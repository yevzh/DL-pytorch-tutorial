{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout丢弃法缓解过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from collections import namedtuple\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchtext\n",
    "import torchtext.vocab as Vocab\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, root='data/FashionMNIST'):\n",
    "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
    "    trans = []\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    trans.append(torchvision.transforms.ToTensor())\n",
    "    \n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n",
    "    if sys.platform.startswith('win'):\n",
    "        num_workers = 0  # 0表示不用额外的进程来加速读取数据\n",
    "    else:\n",
    "        num_workers = 4\n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "已知样本输入为28×28像素，共10个类别。则softmax回归的w,b分别为784×10和1×10矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad=True)\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad=True)\n",
    "W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\n",
    "b3 = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 运算和激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp=x.exp()\n",
    "    tot=x_exp.sum(dim=1,keepdim=True) #表示对第1维（行）求和且保持维度\n",
    "    return x_exp / tot\n",
    "\n",
    "def relu(x):\n",
    "    return torch.max(input=x,other=torch.tensor(0.0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现`dropout`函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X=X.float()\n",
    "    assert 0<=drop_prob<=1\n",
    "    keep_prob = 1- drop_prob\n",
    "    \n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape)< keep_prob).float()\n",
    "    \n",
    "    return mask*X/keep_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2,0.4\n",
    "def net(x,is_training=True):\n",
    "    H1=relu(torch.mm(x.view(-1,num_inputs),W1)+b1)\n",
    "    if is_training:\n",
    "        H1=dropout(H1,drop_prob1)\n",
    "    H2=relu(torch.mm(H1,W2)+b2)\n",
    "    if is_training:\n",
    "        H2=dropout(H2,drop_prob2)\n",
    "    return softmax(torch.mm(H2,W3)+b3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数和优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_hat,y):\n",
    "    return -torch.log(y_hat.gather(1,y.view(-1,1)))\n",
    "    # torch.gather按索引取数\n",
    "    # 如标签为y=[2,0]，对应真实概率为[0,0,1,...][1,0,0,...]，则从y_hat中取y.view（将y倒置）的数参与计算\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size \n",
    "        \n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(net, torch.nn.Module):\n",
    "            net.eval() # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            net.train() # 改回训练模式\n",
    "        else: # 自定义的模型\n",
    "            if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                # 将is_training设置成False\n",
    "                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "            else:\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.1\n",
    "def train(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "        for X,y in train_iter:\n",
    "            y_hat=net(X)\n",
    "            l=loss(y_hat,y).sum()\n",
    "\n",
    "            # w和b梯度清零\n",
    "            for param in params:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            # 计算loss函数梯度，反向传播\n",
    "            l.backward()\n",
    "            \n",
    "            # 梯度下降\n",
    "            sgd(params,lr,batch_size) \n",
    "               \n",
    "            # loss和精确度加和\n",
    "            train_l_sum+=l.item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "            n+=y.shape[0]\n",
    "        test_acc=evaluate_accuracy(test_iter,net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f' \n",
    "            % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.6861, train acc 0.751, test acc 0.786\n",
      "epoch 2, loss 0.6017, train acc 0.786, test acc 0.808\n",
      "epoch 3, loss 0.5407, train acc 0.808, test acc 0.823\n",
      "epoch 4, loss 0.5001, train acc 0.823, test acc 0.832\n",
      "epoch 5, loss 0.4689, train acc 0.834, test acc 0.840\n",
      "epoch 6, loss 0.4463, train acc 0.841, test acc 0.836\n",
      "epoch 7, loss 0.4279, train acc 0.847, test acc 0.847\n",
      "epoch 8, loss 0.4139, train acc 0.852, test acc 0.851\n",
      "epoch 9, loss 0.4011, train acc 0.857, test acc 0.855\n",
      "epoch 10, loss 0.3910, train acc 0.861, test acc 0.847\n"
     ]
    }
   ],
   "source": [
    "train (net, train_iter, test_iter, loss, num_epochs, batch_size, [W1, b1, W2, b2, W3, b3], lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9bb90c8c31d02c70dc2c25559c9d82f9e11e69c39856b10e40706be7daafd48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
