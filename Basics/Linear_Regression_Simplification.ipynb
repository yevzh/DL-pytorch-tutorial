{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  生成数据集\n",
    "我们生成与上一节相同的数据集，其中`features`是训练数据特征，`labels`是标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据\n",
    "PyTorch提供了`data`包来读取数据。由于`data`常用作变量名，我们将导入的`data`模块用`Data`代替。在每一次迭代中，我们将随机读取包含10个样本的小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "batch_size = 10\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "# 随机读取小批量\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的`data_iter`和上一节中的一样。让我们读取并打印小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0562e+00,  4.8149e-01],\n",
      "        [ 3.6190e-02, -1.2309e+00],\n",
      "        [-2.4813e-01,  1.0104e+00],\n",
      "        [-9.7184e-02, -1.5699e-01],\n",
      "        [ 2.4868e+00,  3.3901e-02],\n",
      "        [ 2.1493e-03,  1.2981e-01],\n",
      "        [ 3.2738e-01,  1.0248e-02],\n",
      "        [-9.7520e-01,  5.0003e-01],\n",
      "        [ 3.5258e-02,  4.7666e-01],\n",
      "        [-1.7466e+00, -7.5044e-01]]) tensor([4.6759, 8.4641, 0.2703, 4.5533, 9.0739, 3.7757, 4.8310, 0.5364, 2.6594,\n",
      "        3.2815])\n",
      "tensor([[ 1.0704, -0.6758],\n",
      "        [ 1.4635,  0.3698],\n",
      "        [-0.7128,  1.5996],\n",
      "        [-0.6461,  0.0994],\n",
      "        [ 0.3579,  1.5187],\n",
      "        [-0.2977,  0.6956],\n",
      "        [-0.2678, -0.9193],\n",
      "        [ 0.2822,  1.5234],\n",
      "        [ 1.8422,  1.2524],\n",
      "        [-0.7927, -0.2870]]) tensor([ 8.6254,  5.8679, -2.6555,  2.5857, -0.2393,  1.2363,  6.7883, -0.4112,\n",
      "         3.6297,  3.5711])\n",
      "tensor([[ 0.3488, -0.8462],\n",
      "        [ 0.8871, -0.8729],\n",
      "        [-1.6214, -1.7935],\n",
      "        [-1.3591, -0.1726],\n",
      "        [ 0.2007, -0.7167],\n",
      "        [-0.3125, -0.6132],\n",
      "        [-0.1043,  0.2005],\n",
      "        [ 0.7068,  1.9272],\n",
      "        [-1.0929, -1.7396],\n",
      "        [ 0.2689, -0.2325]]) tensor([ 7.7799,  8.9440,  7.0515,  2.0551,  7.0477,  5.6620,  3.3130, -0.9334,\n",
      "         7.9417,  5.5292])\n",
      "tensor([[-1.8361, -0.2637],\n",
      "        [-1.3876, -0.8269],\n",
      "        [-0.2265, -0.9689],\n",
      "        [-0.3112,  1.4805],\n",
      "        [ 1.3256, -0.0316],\n",
      "        [ 1.0352,  0.4504],\n",
      "        [ 0.0482,  1.4520],\n",
      "        [ 0.0817,  0.1382],\n",
      "        [-0.6651,  1.3968],\n",
      "        [ 1.9847,  1.5811]]) tensor([ 1.4171,  4.2508,  7.0565, -1.4704,  6.9584,  4.7399, -0.6372,  3.8950,\n",
      "        -1.8777,  2.7903])\n",
      "tensor([[-0.3660,  0.9703],\n",
      "        [-0.2154, -0.2341],\n",
      "        [ 0.7921,  0.6734],\n",
      "        [ 1.0409,  0.6576],\n",
      "        [-1.1485, -0.0399],\n",
      "        [-0.1899,  0.4050],\n",
      "        [-1.9213,  0.2602],\n",
      "        [-2.3321,  1.1986],\n",
      "        [-1.4627, -0.0992],\n",
      "        [-0.3119, -0.7658]]) tensor([ 0.1586,  4.5720,  3.5052,  4.0420,  2.0399,  2.4302, -0.5243, -4.5260,\n",
      "         1.6149,  6.1756])\n",
      "tensor([[-1.7178, -0.6449],\n",
      "        [ 0.7090, -1.3879],\n",
      "        [-2.9078, -1.6387],\n",
      "        [ 1.8689, -0.7843],\n",
      "        [-0.4253, -1.3927],\n",
      "        [ 0.6192, -0.2221],\n",
      "        [ 0.0270,  0.3850],\n",
      "        [ 1.1551, -0.4216],\n",
      "        [-0.0731,  0.1159],\n",
      "        [ 0.8776,  0.1364]]) tensor([ 2.9451, 10.3296,  3.9571, 10.6090,  8.0869,  6.1925,  2.9502,  7.9536,\n",
      "         3.6688,  5.4679])\n",
      "tensor([[ 0.1536, -0.6769],\n",
      "        [ 0.2219, -1.2991],\n",
      "        [-0.5751,  1.2307],\n",
      "        [-1.3309,  0.3723],\n",
      "        [-0.0201,  1.0373],\n",
      "        [-0.3059, -0.0603],\n",
      "        [ 0.6432,  1.6733],\n",
      "        [ 1.2522,  1.0961],\n",
      "        [ 0.5968, -0.6946],\n",
      "        [-2.1516, -1.0806]]) tensor([ 6.8057,  9.0573, -1.1183,  0.2535,  0.6458,  3.7877, -0.2010,  2.9800,\n",
      "         7.7659,  3.5515])\n",
      "tensor([[ 0.9476, -0.2346],\n",
      "        [ 2.1080, -0.9451],\n",
      "        [ 0.0122,  0.1578],\n",
      "        [ 0.8599, -1.0330],\n",
      "        [ 0.8211, -0.7341],\n",
      "        [ 0.8471,  0.6367],\n",
      "        [-1.1599, -0.9913],\n",
      "        [ 1.4601, -0.2139],\n",
      "        [-0.5197, -1.2449],\n",
      "        [ 1.0290,  0.2526]]) tensor([ 6.8842, 11.6254,  3.6957,  9.4100,  8.3381,  3.7288,  5.2491,  7.8684,\n",
      "         7.3979,  5.3893])\n",
      "tensor([[-0.3779, -0.0565],\n",
      "        [-1.6994,  3.0462],\n",
      "        [ 0.8124,  0.6030],\n",
      "        [ 0.0033, -0.0503],\n",
      "        [ 0.1167, -1.4842],\n",
      "        [-0.5249, -0.4621],\n",
      "        [-1.7233,  0.2143],\n",
      "        [ 2.4434,  0.1845],\n",
      "        [ 1.6160, -0.1455],\n",
      "        [-0.3406,  1.4365]]) tensor([ 3.6197, -9.5596,  3.7932,  4.3776,  9.4767,  4.7365,  0.0339,  8.4505,\n",
      "         7.9437, -1.3760])\n",
      "tensor([[ 0.2316, -0.1117],\n",
      "        [ 0.6763,  0.5281],\n",
      "        [-0.6170,  1.3518],\n",
      "        [-0.5067, -0.4753],\n",
      "        [ 0.1731, -1.1820],\n",
      "        [ 2.0729,  1.3071],\n",
      "        [-1.0549,  0.4424],\n",
      "        [-0.4526,  0.3083],\n",
      "        [ 1.9580,  0.0583],\n",
      "        [-0.2630, -0.5132]]) tensor([ 5.0525,  3.7534, -1.6194,  4.7879,  8.5433,  3.9053,  0.5666,  2.2415,\n",
      "         7.9136,  5.4281])\n",
      "tensor([[ 1.1638,  0.6475],\n",
      "        [ 0.9579,  0.6402],\n",
      "        [ 0.9656,  0.7992],\n",
      "        [ 1.7284, -0.9127],\n",
      "        [-0.3302,  0.0641],\n",
      "        [ 0.2292, -0.6621],\n",
      "        [ 2.1276, -0.6893],\n",
      "        [-0.8112,  0.8934],\n",
      "        [ 1.4160, -1.6218],\n",
      "        [ 0.8368, -1.4739]]) tensor([ 4.3109,  3.9390,  3.4074, 10.7649,  3.3209,  6.9034, 10.8134, -0.4682,\n",
      "        12.5537, 10.8805])\n",
      "tensor([[ 0.6412,  0.4968],\n",
      "        [ 0.1037,  0.4571],\n",
      "        [ 0.3863,  0.3358],\n",
      "        [ 1.1661, -0.8346],\n",
      "        [ 0.9232,  2.2284],\n",
      "        [ 1.6022,  0.9121],\n",
      "        [ 0.8165,  0.2173],\n",
      "        [ 0.0419, -1.9857],\n",
      "        [-0.6450, -0.1891],\n",
      "        [ 0.0890, -0.5649]]) tensor([ 3.7878,  2.8633,  3.8391,  9.3866, -1.5053,  4.3034,  5.1026, 11.0373,\n",
      "         3.5476,  6.3128])\n",
      "tensor([[-1.0927,  1.6343],\n",
      "        [-0.7559, -0.7419],\n",
      "        [ 1.4695, -1.1128],\n",
      "        [-0.9568, -0.6092],\n",
      "        [ 0.3932, -0.4000],\n",
      "        [ 1.0251, -0.3869],\n",
      "        [ 0.5380,  0.8174],\n",
      "        [-2.2332, -1.2512],\n",
      "        [ 0.0544,  1.0442],\n",
      "        [ 0.8222, -1.6661]]) tensor([-3.5352,  5.2228, 10.9154,  4.3780,  6.3389,  7.5655,  2.5005,  3.9881,\n",
      "         0.7651, 11.5078])\n",
      "tensor([[-1.4434,  0.6668],\n",
      "        [ 1.1008,  0.4026],\n",
      "        [ 1.1302, -1.5432],\n",
      "        [-0.4080, -0.5487],\n",
      "        [-1.0165,  0.8142],\n",
      "        [-0.8045,  2.1151],\n",
      "        [-0.2564,  0.9970],\n",
      "        [-0.1214,  1.3854],\n",
      "        [ 0.6452, -1.9791],\n",
      "        [-1.2718, -0.1918]]) tensor([-0.9558,  5.0363, 11.7029,  5.2495, -0.6160, -4.5972,  0.2952, -0.7789,\n",
      "        12.2050,  2.3284])\n",
      "tensor([[ 2.3146,  0.1752],\n",
      "        [-0.5002,  0.0428],\n",
      "        [-0.1938, -1.1836],\n",
      "        [ 0.5958,  0.3538],\n",
      "        [ 0.7623,  0.5195],\n",
      "        [-0.1061,  1.2070],\n",
      "        [-2.0770, -0.0120],\n",
      "        [-0.0630, -0.3549],\n",
      "        [-0.0425, -0.0327],\n",
      "        [-0.0970,  1.4293]]) tensor([ 8.2382,  3.0688,  7.8274,  4.1945,  3.9770, -0.1345,  0.0807,  5.2959,\n",
      "         4.2213, -0.8617])\n",
      "tensor([[-0.3659, -0.8377],\n",
      "        [ 0.7349, -0.3427],\n",
      "        [ 0.4708,  0.3028],\n",
      "        [-0.7456,  0.6912],\n",
      "        [-0.2081, -0.6221],\n",
      "        [-0.5466,  0.0031],\n",
      "        [ 2.2273,  0.1456],\n",
      "        [-0.2512,  0.0917],\n",
      "        [ 0.0883, -1.4552],\n",
      "        [ 0.1479,  1.0588]]) tensor([6.3294, 6.8361, 4.1177, 0.3627, 5.9187, 3.0821, 8.1661, 3.3902, 9.3244,\n",
      "        0.8893])\n",
      "tensor([[ 0.7841, -0.3496],\n",
      "        [-0.2287,  0.6160],\n",
      "        [-0.3415,  0.4096],\n",
      "        [-0.0506, -1.3110],\n",
      "        [-0.5776, -0.2773],\n",
      "        [-1.5719, -0.3491],\n",
      "        [ 0.4211,  0.9893],\n",
      "        [-0.4282,  0.7438],\n",
      "        [ 0.0238, -0.1027],\n",
      "        [-0.7027,  0.2770]]) tensor([6.9416, 1.6467, 2.1179, 8.5482, 3.9978, 2.2481, 1.6802, 0.8188, 4.5889,\n",
      "        1.8504])\n",
      "tensor([[-0.1687, -1.1573],\n",
      "        [ 0.2231, -0.5381],\n",
      "        [-0.8549,  0.4717],\n",
      "        [ 0.9365,  0.3734],\n",
      "        [ 0.2659,  0.1013],\n",
      "        [-0.2684,  1.1499],\n",
      "        [ 0.7835, -2.4255],\n",
      "        [ 0.6104, -2.7277],\n",
      "        [ 1.8335, -0.9670],\n",
      "        [-0.3945, -0.2283]]) tensor([ 7.7992,  6.4796,  0.8801,  4.8070,  4.3850, -0.2291, 14.0220, 14.6913,\n",
      "        11.1489,  4.2023])\n",
      "tensor([[ 0.0561, -0.8683],\n",
      "        [ 0.9143,  0.7143],\n",
      "        [-0.4986, -0.2430],\n",
      "        [ 0.3492, -0.9881],\n",
      "        [-0.3216, -0.4083],\n",
      "        [ 1.0276, -0.5173],\n",
      "        [-0.0415, -0.2322],\n",
      "        [-0.9654, -1.1928],\n",
      "        [-1.5336,  0.4597],\n",
      "        [ 0.5170,  0.9056]]) tensor([ 7.2639,  3.5957,  4.0392,  8.2514,  4.9380,  8.0194,  4.8975,  6.3186,\n",
      "        -0.4336,  2.1426])\n",
      "tensor([[ 0.1517, -0.1299],\n",
      "        [ 0.1334,  1.2346],\n",
      "        [ 1.0551, -0.4620],\n",
      "        [-1.4859,  1.1866],\n",
      "        [ 0.2825, -0.6354],\n",
      "        [-0.1635,  0.0080],\n",
      "        [ 1.3470,  0.2045],\n",
      "        [-0.7110,  0.2615],\n",
      "        [ 0.7882, -0.4882],\n",
      "        [ 0.2957, -0.7985]]) tensor([ 4.9379,  0.2576,  7.8862, -2.8083,  6.9131,  3.8330,  6.2013,  1.8900,\n",
      "         7.4233,  7.4985])\n",
      "tensor([[ 0.5846, -0.0398],\n",
      "        [-1.3420, -1.0056],\n",
      "        [ 0.6353, -1.3880],\n",
      "        [ 0.3162,  0.8495],\n",
      "        [ 1.5953,  0.4919],\n",
      "        [-0.7518,  1.0385],\n",
      "        [ 0.0936,  0.9934],\n",
      "        [-0.8040,  0.3218],\n",
      "        [ 1.1903, -1.2507],\n",
      "        [ 0.2400,  0.5174]]) tensor([ 5.5023,  4.9285, 10.1994,  1.9463,  5.7302, -0.8457,  0.9888,  1.4947,\n",
      "        10.8290,  2.9338])\n",
      "tensor([[-1.1658, -0.1260],\n",
      "        [-0.5352, -2.7923],\n",
      "        [-0.1844,  0.5777],\n",
      "        [-0.5210,  0.7559],\n",
      "        [-0.4332, -1.3112],\n",
      "        [-1.4703,  0.0044],\n",
      "        [-0.7377, -0.9347],\n",
      "        [ 0.2842, -0.6336],\n",
      "        [ 1.0556,  0.1782],\n",
      "        [ 1.2862, -0.5136]]) tensor([ 2.3129, 12.6172,  1.8685,  0.5952,  7.7995,  1.2310,  5.8975,  6.9159,\n",
      "         5.7069,  8.5090])\n",
      "tensor([[ 0.2029, -1.0716],\n",
      "        [-1.2561,  0.3975],\n",
      "        [-1.0855, -0.2891],\n",
      "        [-1.2293,  0.8164],\n",
      "        [ 0.5730, -1.1243],\n",
      "        [ 0.9661,  0.4470],\n",
      "        [-1.4739,  0.7257],\n",
      "        [-0.3494,  1.5063],\n",
      "        [-0.9584,  1.4680],\n",
      "        [ 1.5740, -0.5768]]) tensor([ 8.2465,  0.3259,  3.0166, -1.0376,  9.1654,  4.5974, -1.2264, -1.6179,\n",
      "        -2.7089,  9.3027])\n",
      "tensor([[ 0.0700,  0.4254],\n",
      "        [-0.2151,  0.9580],\n",
      "        [-0.4517,  0.5713],\n",
      "        [ 2.0984,  0.2368],\n",
      "        [-0.2618, -0.1558],\n",
      "        [ 3.2541,  0.3924],\n",
      "        [-0.9826,  0.1279],\n",
      "        [-1.2969,  0.1189],\n",
      "        [ 0.2668, -0.9685],\n",
      "        [-0.6730, -0.1272]]) tensor([2.9080, 0.5162, 1.3522, 7.5754, 4.2154, 9.3672, 1.7996, 1.1953, 8.0340,\n",
      "        3.2961])\n",
      "tensor([[ 0.3242, -0.3357],\n",
      "        [-1.0219, -1.6420],\n",
      "        [-2.3288, -1.1666],\n",
      "        [ 0.5374, -1.7323],\n",
      "        [-0.0767, -0.6398],\n",
      "        [-1.6199, -0.7072],\n",
      "        [ 0.4238,  2.7691],\n",
      "        [ 1.0328, -0.0707],\n",
      "        [-1.5554, -1.1301],\n",
      "        [ 1.0192,  1.5057]]) tensor([ 5.9658,  7.7285,  3.5023, 11.1652,  6.2191,  3.3556, -4.3752,  6.5159,\n",
      "         4.9344,  1.1254])\n",
      "tensor([[-0.2485, -0.1295],\n",
      "        [-1.1595, -1.1545],\n",
      "        [-0.5118,  0.9415],\n",
      "        [-1.5845, -0.1325],\n",
      "        [ 0.9552,  2.3414],\n",
      "        [ 0.7912,  0.7972],\n",
      "        [-1.0322, -0.9635],\n",
      "        [ 0.8057,  0.3241],\n",
      "        [-0.5943, -1.8130],\n",
      "        [ 0.0961,  0.1519]]) tensor([ 4.1326,  5.7928, -0.0235,  1.4838, -1.8538,  3.0741,  5.4272,  4.7114,\n",
      "         9.1737,  3.8716])\n",
      "tensor([[-0.8726,  0.0233],\n",
      "        [ 0.5205, -0.7501],\n",
      "        [ 1.2608,  0.0394],\n",
      "        [-1.8177, -0.2190],\n",
      "        [-0.6687, -1.1449],\n",
      "        [-1.3182, -0.0598],\n",
      "        [-0.4937,  2.2047],\n",
      "        [-0.5650, -0.0451],\n",
      "        [-2.6599,  0.1161],\n",
      "        [ 0.7921,  0.3442]]) tensor([ 2.3860,  7.7840,  6.5945,  1.3006,  6.7429,  1.7608, -4.2800,  3.2368,\n",
      "        -1.5060,  4.6197])\n",
      "tensor([[-0.5774,  0.1415],\n",
      "        [-0.5080, -0.1905],\n",
      "        [-0.8191,  0.3955],\n",
      "        [-0.9657,  0.6666],\n",
      "        [ 1.7969, -0.4737],\n",
      "        [ 1.3122,  1.2854],\n",
      "        [ 0.7880,  0.0073],\n",
      "        [-1.0891, -0.3560],\n",
      "        [ 0.8396, -0.2997],\n",
      "        [-0.0200, -0.3649]]) tensor([2.5467e+00, 3.8529e+00, 1.2214e+00, 3.9806e-03, 9.3926e+00, 2.4602e+00,\n",
      "        5.7438e+00, 3.2374e+00, 6.9039e+00, 5.3891e+00])\n",
      "tensor([[-1.3108,  0.8916],\n",
      "        [-0.4468, -1.7896],\n",
      "        [ 1.9494,  0.4631],\n",
      "        [-1.9509,  1.3230],\n",
      "        [-2.0284, -0.3082],\n",
      "        [ 0.1361, -0.1163],\n",
      "        [-1.4673,  0.6153],\n",
      "        [-0.7447,  2.8325],\n",
      "        [ 0.0114, -0.7571],\n",
      "        [-0.8344, -0.3005]]) tensor([-1.4464,  9.3838,  6.5320, -4.2019,  1.1959,  4.8789, -0.8201, -6.9283,\n",
      "         6.7876,  3.5785])\n",
      "tensor([[ 0.6036, -1.7708],\n",
      "        [-0.0465, -0.0958],\n",
      "        [-1.3724,  0.6645],\n",
      "        [ 0.3281, -0.8044],\n",
      "        [ 0.9244,  1.5415],\n",
      "        [ 0.9993, -0.5116],\n",
      "        [-0.4364,  2.5003],\n",
      "        [ 1.0471, -0.0861],\n",
      "        [-1.8266, -0.5084],\n",
      "        [ 0.2729,  0.1228]]) tensor([11.4221,  4.4365, -0.8090,  7.6025,  0.8090,  7.9370, -5.1883,  6.5814,\n",
      "         2.2746,  4.3233])\n",
      "tensor([[ 2.0674, -0.9326],\n",
      "        [ 0.0751, -0.8218],\n",
      "        [-1.3507,  0.0537],\n",
      "        [-1.6440,  0.9738],\n",
      "        [ 0.6877, -0.1481],\n",
      "        [ 1.8498, -1.0340],\n",
      "        [-1.2939, -0.6373],\n",
      "        [ 0.3558, -0.0311],\n",
      "        [-0.4410, -0.8226],\n",
      "        [ 2.4805,  0.6478]]) tensor([11.4779,  7.1345,  1.3153, -2.4192,  6.0637, 11.4203,  3.7766,  5.0218,\n",
      "         6.1092,  6.9612])\n",
      "tensor([[ 0.3162,  0.0597],\n",
      "        [ 0.5259, -2.3777],\n",
      "        [ 0.4505, -0.8670],\n",
      "        [ 0.6764, -1.7454],\n",
      "        [-1.2596,  0.4543],\n",
      "        [ 0.6374,  0.9612],\n",
      "        [ 1.8325,  0.1685],\n",
      "        [-0.6118, -1.3462],\n",
      "        [ 2.7014,  0.6458],\n",
      "        [ 0.9304,  0.6026]]) tensor([ 4.6199, 13.3456,  8.0508, 11.4800,  0.1453,  2.2030,  7.2864,  7.5587,\n",
      "         7.4025,  4.0104])\n",
      "tensor([[ 0.8990, -0.7444],\n",
      "        [ 1.5175,  1.7465],\n",
      "        [-1.1814, -0.5251],\n",
      "        [-1.1491, -0.6559],\n",
      "        [-0.5762, -0.5206],\n",
      "        [ 1.1852,  0.8894],\n",
      "        [ 0.4276,  0.0712],\n",
      "        [ 0.0620, -1.4494],\n",
      "        [-0.3178,  1.9416],\n",
      "        [ 0.1987, -0.2933]]) tensor([ 8.5308,  1.2983,  3.6247,  4.1279,  4.8119,  3.5654,  4.8170,  9.2276,\n",
      "        -3.0472,  5.5602])\n",
      "tensor([[-1.5434,  1.0107],\n",
      "        [-0.6653,  0.0528],\n",
      "        [-3.0237, -0.2176],\n",
      "        [-0.5722, -1.6191],\n",
      "        [-1.0185,  0.8887],\n",
      "        [-1.3353,  1.2280],\n",
      "        [ 0.9047,  1.2782],\n",
      "        [-1.4957, -0.1315],\n",
      "        [-1.3636,  0.0439],\n",
      "        [ 1.8497, -0.6900]]) tensor([-2.3355,  2.7067, -1.1027,  8.5710, -0.8680, -2.6604,  1.6645,  1.6575,\n",
      "         1.3389, 10.2397])\n",
      "tensor([[-0.3959,  0.7771],\n",
      "        [ 0.6153,  1.0276],\n",
      "        [-1.2728,  0.4070],\n",
      "        [-1.1333,  0.5652],\n",
      "        [ 0.0176,  0.8080],\n",
      "        [-0.3971,  0.0083],\n",
      "        [-0.3193, -0.0883],\n",
      "        [ 1.3068, -0.9613],\n",
      "        [-0.3684, -0.3614],\n",
      "        [ 0.9593, -1.5762]]) tensor([7.8603e-01, 1.9219e+00, 2.7926e-01, 8.7972e-03, 1.4906e+00, 3.3867e+00,\n",
      "        3.8482e+00, 1.0093e+01, 4.6713e+00, 1.1462e+01])\n",
      "tensor([[ 0.1047,  0.7472],\n",
      "        [-0.0145, -0.9269],\n",
      "        [ 0.9553,  1.0307],\n",
      "        [ 0.1090,  0.0976],\n",
      "        [-0.6975, -0.0373],\n",
      "        [-1.4863,  0.6647],\n",
      "        [-0.5811,  2.3428],\n",
      "        [-0.2013, -0.7736],\n",
      "        [ 0.8726, -0.5232],\n",
      "        [-0.0168,  1.5698]]) tensor([ 1.8677,  7.3205,  2.5977,  4.0921,  2.9161, -1.0323, -4.9317,  6.4278,\n",
      "         7.7325, -1.1709])\n",
      "tensor([[-0.3496, -0.0640],\n",
      "        [-0.5562,  1.5565],\n",
      "        [ 2.1694,  0.8092],\n",
      "        [ 0.0285, -0.5328],\n",
      "        [ 0.5841,  1.9353],\n",
      "        [-0.0577,  0.2544],\n",
      "        [-0.4000, -1.0331],\n",
      "        [ 0.0961, -2.2990],\n",
      "        [-1.0406, -0.5840],\n",
      "        [-1.0035,  1.2555]]) tensor([ 3.7045, -2.2047,  5.7704,  6.0453, -1.2138,  3.2121,  6.8988, 12.2085,\n",
      "         4.1254, -2.0826])\n",
      "tensor([[ 0.8522,  1.0364],\n",
      "        [ 0.4906, -0.3509],\n",
      "        [ 0.0346,  0.7321],\n",
      "        [-0.9578, -0.4046],\n",
      "        [-0.4804, -1.4717],\n",
      "        [ 0.7415, -0.1257],\n",
      "        [-0.4210,  1.4892],\n",
      "        [-0.1425, -0.2930],\n",
      "        [ 1.4846,  2.2312],\n",
      "        [ 0.4011, -0.1476]]) tensor([ 2.3566,  6.3728,  1.7891,  3.6617,  8.2370,  6.1152, -1.6981,  4.9118,\n",
      "        -0.3997,  5.4672])\n",
      "tensor([[ 1.2241, -0.2184],\n",
      "        [-0.5225, -0.0595],\n",
      "        [-1.0131, -1.2033],\n",
      "        [-2.0179,  0.1843],\n",
      "        [ 0.4417,  1.1052],\n",
      "        [ 0.7122, -0.1594],\n",
      "        [-0.7341,  0.0950],\n",
      "        [ 0.3120, -0.5281],\n",
      "        [-0.3441,  1.6948],\n",
      "        [ 1.9997,  0.8748]]) tensor([ 7.3780,  3.3663,  6.2600, -0.4587,  1.3130,  6.1739,  2.4183,  6.6156,\n",
      "        -2.2451,  5.2330])\n",
      "tensor([[ 0.5337,  0.4774],\n",
      "        [-0.9038,  2.3880],\n",
      "        [ 0.5317,  2.2842],\n",
      "        [-1.6958,  0.0597],\n",
      "        [-0.6253, -0.9375],\n",
      "        [ 0.0852,  2.7597],\n",
      "        [ 0.3562, -0.6383],\n",
      "        [-0.5581, -0.8109],\n",
      "        [ 1.7571, -0.8180],\n",
      "        [-0.0605, -0.0569]]) tensor([ 3.6389, -5.7202, -2.4963,  0.5988,  6.1331, -4.9964,  7.0794,  5.8479,\n",
      "        10.4823,  4.2711])\n",
      "tensor([[ 0.7300,  0.2149],\n",
      "        [-0.6039,  1.1042],\n",
      "        [ 0.9617,  1.1851],\n",
      "        [ 2.6705, -1.8512],\n",
      "        [ 1.0446,  1.1451],\n",
      "        [ 0.6232,  1.1954],\n",
      "        [ 2.4216,  0.7019],\n",
      "        [-0.1087, -0.7774],\n",
      "        [ 1.1850,  0.4300],\n",
      "        [ 0.7013,  0.0837]]) tensor([ 4.9338, -0.7612,  2.1077, 15.8370,  2.3868,  1.3866,  6.6455,  6.6419,\n",
      "         5.1100,  5.3202])\n",
      "tensor([[ 0.5146, -0.1342],\n",
      "        [-0.6742,  0.4869],\n",
      "        [-0.2355,  0.6898],\n",
      "        [-0.5731, -1.2996],\n",
      "        [ 0.0927, -0.7081],\n",
      "        [ 1.1155, -1.7932],\n",
      "        [ 1.5871, -0.9977],\n",
      "        [ 0.5390, -0.4892],\n",
      "        [-0.3186,  0.9473],\n",
      "        [-0.7638, -2.3842]]) tensor([ 5.6934,  1.1882,  1.3889,  7.4694,  6.7866, 12.5169, 10.7758,  6.9460,\n",
      "         0.3298, 10.7724])\n",
      "tensor([[-1.1770, -2.6349],\n",
      "        [ 1.3773, -0.2981],\n",
      "        [-0.1256, -0.7251],\n",
      "        [ 1.3387, -1.3619],\n",
      "        [ 0.2117,  0.4053],\n",
      "        [ 0.8482, -0.0997],\n",
      "        [ 1.2252,  1.7185],\n",
      "        [ 0.6381, -0.9153],\n",
      "        [-1.0513,  0.1419],\n",
      "        [-1.6548,  1.6672]]) tensor([10.7993,  7.9658,  6.4072, 11.5254,  3.2481,  6.2304,  0.7974,  8.5846,\n",
      "         1.5984, -4.7959])\n",
      "tensor([[ 0.5067, -0.5777],\n",
      "        [ 0.2183,  2.4465],\n",
      "        [-0.2076,  2.0955],\n",
      "        [-1.4169, -0.1657],\n",
      "        [-0.1691,  1.4501],\n",
      "        [ 1.2428, -0.9891],\n",
      "        [-1.0566, -1.0034],\n",
      "        [-1.0977, -0.2188],\n",
      "        [ 1.8662,  1.9342],\n",
      "        [-0.2967, -0.0325]]) tensor([ 7.1952, -3.6718, -3.3456,  1.9376, -1.0629, 10.0488,  5.4993,  2.7596,\n",
      "         1.3546,  3.7242])\n",
      "tensor([[ 0.6744,  0.2948],\n",
      "        [-0.8049,  0.6283],\n",
      "        [-0.0235, -0.8808],\n",
      "        [-0.4276,  1.4239],\n",
      "        [ 0.3284, -0.5243],\n",
      "        [ 0.3027, -0.6738],\n",
      "        [ 1.6736,  0.5626],\n",
      "        [ 1.1146,  0.6619],\n",
      "        [-0.7115, -0.9964],\n",
      "        [ 0.3385, -0.1935]]) tensor([ 4.5524,  0.4492,  7.1533, -1.5105,  6.6478,  7.0857,  5.6545,  4.1932,\n",
      "         6.1739,  5.5459])\n",
      "tensor([[ 1.3114,  2.4397],\n",
      "        [ 1.1557, -1.2535],\n",
      "        [ 0.4948,  0.2884],\n",
      "        [-0.8601,  1.3508],\n",
      "        [-0.4420, -0.7751],\n",
      "        [ 0.9277,  0.0036],\n",
      "        [ 1.4960,  1.0935],\n",
      "        [ 0.1220, -0.6146],\n",
      "        [ 0.0739, -0.0266],\n",
      "        [-0.2487, -0.4063]]) tensor([-1.4540, 10.7705,  4.1827, -2.1085,  5.9484,  6.0540,  3.4824,  6.5209,\n",
      "         4.4324,  5.0889])\n",
      "tensor([[-0.8766,  2.8648],\n",
      "        [-0.4285,  0.3193],\n",
      "        [-0.1031,  1.2690],\n",
      "        [-0.8755,  0.7546],\n",
      "        [ 0.3758, -0.0633],\n",
      "        [-0.1590,  0.6199],\n",
      "        [ 0.9839,  0.2990],\n",
      "        [ 0.6004, -1.4023],\n",
      "        [-1.0793,  0.1420],\n",
      "        [ 0.1041,  0.0175]]) tensor([-7.2815,  2.2641, -0.3221, -0.0987,  5.1562,  1.7536,  5.1606, 10.1747,\n",
      "         1.5666,  4.3588])\n",
      "tensor([[ 0.9060,  0.9716],\n",
      "        [ 0.9852,  1.1647],\n",
      "        [-0.0621, -0.7036],\n",
      "        [-1.2035, -1.1296],\n",
      "        [ 1.3255, -0.3699],\n",
      "        [-0.7047, -1.0405],\n",
      "        [-1.3235,  1.9215],\n",
      "        [ 1.7114,  0.2182],\n",
      "        [-0.5260, -0.2726],\n",
      "        [-0.0987,  0.6799]]) tensor([ 2.6971,  2.1912,  6.4718,  5.6347,  8.1234,  6.3200, -4.9748,  6.8977,\n",
      "         4.0861,  1.6723])\n",
      "tensor([[ 1.8237, -1.2458],\n",
      "        [-0.4641,  0.1997],\n",
      "        [-0.6838,  0.4674],\n",
      "        [ 1.3062, -0.8546],\n",
      "        [ 0.4332, -0.4783],\n",
      "        [-0.4220, -0.4195],\n",
      "        [ 0.2017,  0.3690],\n",
      "        [ 0.0701, -0.2171],\n",
      "        [ 1.1091,  0.3106],\n",
      "        [ 0.5944, -0.7298]]) tensor([12.0990,  2.5948,  1.2341,  9.7172,  6.6759,  4.7858,  3.3569,  5.0840,\n",
      "         5.3665,  7.8544])\n",
      "tensor([[-0.0798, -0.8845],\n",
      "        [ 0.0828, -0.4031],\n",
      "        [-0.6952,  1.4229],\n",
      "        [ 0.4128,  1.5790],\n",
      "        [-0.9313,  1.8400],\n",
      "        [ 2.0439,  0.9987],\n",
      "        [-0.0111, -0.3209],\n",
      "        [ 0.2312,  1.3997],\n",
      "        [-0.7511,  1.2891],\n",
      "        [-0.8363,  0.4004]]) tensor([ 7.0412,  5.7211, -2.0277, -0.3585, -3.9268,  4.8878,  5.2459, -0.0839,\n",
      "        -1.6826,  1.1753])\n",
      "tensor([[ 0.8666,  1.0096],\n",
      "        [-0.2314,  0.7773],\n",
      "        [ 1.2946,  0.8463],\n",
      "        [ 1.2442,  0.1812],\n",
      "        [ 0.4368,  1.0172],\n",
      "        [-0.7151, -0.4690],\n",
      "        [-0.0288, -0.8841],\n",
      "        [-0.1498, -0.9773],\n",
      "        [-0.0557, -0.0398],\n",
      "        [ 0.1404,  1.2708]]) tensor([2.4903, 1.0901, 3.9132, 6.0601, 1.5996, 4.3656, 7.1482, 7.2386, 4.2238,\n",
      "        0.1671])\n",
      "tensor([[ 1.2418,  0.1338],\n",
      "        [-1.3112, -1.6514],\n",
      "        [-0.1941, -0.6175],\n",
      "        [ 0.0548, -1.2783],\n",
      "        [ 0.4127,  1.8034],\n",
      "        [-0.1668, -0.4040],\n",
      "        [-1.1835, -0.1896],\n",
      "        [-0.1090,  0.7971],\n",
      "        [ 1.5202,  0.7393],\n",
      "        [-0.4964, -1.4079]]) tensor([ 6.2265,  7.1946,  5.9214,  8.6548, -1.1097,  5.2613,  2.4814,  1.2897,\n",
      "         4.7198,  7.9879])\n",
      "tensor([[ 0.5064,  0.2376],\n",
      "        [-1.0672, -0.2694],\n",
      "        [-0.1739, -0.0827],\n",
      "        [ 0.1728,  0.1415],\n",
      "        [ 1.5328,  0.8057],\n",
      "        [-1.9915,  1.5094],\n",
      "        [-2.2695,  0.0660],\n",
      "        [-0.6318,  1.4771],\n",
      "        [ 0.2526, -0.5725],\n",
      "        [ 0.8176,  0.0388]]) tensor([ 4.3834,  2.9561,  4.1371,  4.0681,  4.5635, -4.9167, -0.5789, -2.0860,\n",
      "         6.6499,  5.7147])\n",
      "tensor([[-1.2895, -1.9040],\n",
      "        [ 0.4428, -1.2381],\n",
      "        [-0.7585,  0.4092],\n",
      "        [ 0.6203,  0.4623],\n",
      "        [ 0.7202, -0.9348],\n",
      "        [ 0.0563, -1.2319],\n",
      "        [ 0.0228,  0.5773],\n",
      "        [ 0.4099, -0.1531],\n",
      "        [ 0.0310, -1.5277],\n",
      "        [ 0.6346,  0.3440]]) tensor([8.0887, 9.3050, 1.3022, 3.8627, 8.8228, 8.5037, 2.2736, 5.5263, 9.4636,\n",
      "        4.2904])\n",
      "tensor([[ 0.1114,  1.7869],\n",
      "        [-0.5420,  0.2282],\n",
      "        [-1.3670, -2.4630],\n",
      "        [-0.9674, -1.6187],\n",
      "        [ 0.8117, -0.2125],\n",
      "        [-1.7053,  0.9737],\n",
      "        [-0.1886,  0.1176],\n",
      "        [-2.5586,  1.3723],\n",
      "        [ 1.8784,  0.2503],\n",
      "        [-0.0467, -0.8180]]) tensor([-1.6469,  2.3490,  9.8309,  7.7779,  6.5363, -2.5428,  3.4045, -5.5787,\n",
      "         7.1206,  6.8896])\n",
      "tensor([[ 1.5880, -0.5232],\n",
      "        [ 0.4792, -0.6424],\n",
      "        [-0.0320,  0.1902],\n",
      "        [ 2.3971,  0.1790],\n",
      "        [ 0.3104,  0.1516],\n",
      "        [ 0.1643, -0.5258],\n",
      "        [-0.6409, -0.3997],\n",
      "        [ 2.4303, -0.2129],\n",
      "        [ 0.7861, -0.6032],\n",
      "        [ 0.9098,  0.7708]]) tensor([9.1640, 7.3418, 3.4765, 8.3844, 4.2945, 6.3123, 4.2713, 9.7822, 7.8287,\n",
      "        3.3959])\n",
      "tensor([[-0.1219, -0.3292],\n",
      "        [-0.9166, -0.9563],\n",
      "        [ 0.3777, -0.2005],\n",
      "        [-0.4458, -0.9235],\n",
      "        [-1.3863,  0.3679],\n",
      "        [-0.2555,  0.0047],\n",
      "        [-1.4697,  0.5989],\n",
      "        [-0.3466,  0.7602],\n",
      "        [-0.8609,  0.9740],\n",
      "        [-0.3654,  0.6545]]) tensor([ 5.0796,  5.6252,  5.6313,  6.4404,  0.1844,  3.6613, -0.7851,  0.9264,\n",
      "        -0.8277,  1.2196])\n",
      "tensor([[ 0.7827,  1.3254],\n",
      "        [ 1.1518,  0.0285],\n",
      "        [ 0.3269, -0.9958],\n",
      "        [ 0.2649,  0.9691],\n",
      "        [ 0.1388, -0.2984],\n",
      "        [-1.2930,  0.1018],\n",
      "        [ 0.6010, -0.7609],\n",
      "        [-0.9769,  1.0710],\n",
      "        [ 0.3650, -0.5028],\n",
      "        [ 0.4526, -1.5922]]) tensor([ 1.2598,  6.4110,  8.2490,  1.4579,  5.4914,  1.2721,  7.9877, -1.3806,\n",
      "         6.6392, 10.5255])\n",
      "tensor([[ 6.8337e-01, -5.1817e-01],\n",
      "        [ 6.4242e-03, -1.1279e-01],\n",
      "        [ 4.9393e-01,  9.0917e-01],\n",
      "        [ 2.5363e-01,  3.6535e-04],\n",
      "        [-1.6733e-01, -2.0150e-01],\n",
      "        [ 1.6669e-01,  1.0736e+00],\n",
      "        [-2.0359e+00,  6.6204e-01],\n",
      "        [-6.4684e-01,  4.7692e-01],\n",
      "        [-2.3644e+00, -3.1373e-01],\n",
      "        [ 7.9783e-02, -3.8533e-01]]) tensor([ 7.3265,  4.5860,  2.0985,  4.7125,  4.5389,  0.8901, -2.1145,  1.2787,\n",
      "         0.5301,  5.6706])\n",
      "tensor([[-1.2728, -0.4177],\n",
      "        [ 0.4505, -1.1565],\n",
      "        [ 1.5788,  0.6337],\n",
      "        [ 0.1072,  0.3514],\n",
      "        [ 0.5677,  0.2127],\n",
      "        [ 0.4892,  0.1964],\n",
      "        [ 2.5941, -1.0463],\n",
      "        [-0.3925, -0.3626],\n",
      "        [ 0.6786, -0.7034],\n",
      "        [ 0.5862,  0.9679]]) tensor([ 3.0663,  9.0355,  5.1970,  3.2262,  4.6133,  4.5095, 12.9598,  4.6503,\n",
      "         7.9295,  2.0676])\n",
      "tensor([[-0.0084, -2.2540],\n",
      "        [-1.6131,  0.5009],\n",
      "        [-0.6734, -1.7429],\n",
      "        [-1.5572, -1.1248],\n",
      "        [-0.3469,  1.0061],\n",
      "        [-0.9180, -0.0344],\n",
      "        [ 0.9403, -0.0430],\n",
      "        [-0.4355,  0.2509],\n",
      "        [-3.0916, -0.2506],\n",
      "        [-1.2138,  1.1816]]) tensor([11.8586, -0.7348,  8.7868,  4.9125,  0.0680,  2.5003,  6.2159,  2.4647,\n",
      "        -1.1314, -2.2293])\n",
      "tensor([[-0.1024,  0.7237],\n",
      "        [ 0.4034,  0.5637],\n",
      "        [ 0.8910, -0.5323],\n",
      "        [ 1.7216,  0.1153],\n",
      "        [-0.9033, -1.1901],\n",
      "        [ 0.9488,  0.2480],\n",
      "        [-1.5091, -0.3030],\n",
      "        [-1.1113,  0.2352],\n",
      "        [ 0.0722, -0.9012],\n",
      "        [ 0.8681, -0.6928]]) tensor([1.5375, 3.0696, 7.7984, 7.2486, 6.4465, 5.2555, 2.2126, 1.1856, 7.3992,\n",
      "        8.2888])\n",
      "tensor([[ 1.3012,  0.0196],\n",
      "        [ 0.7181, -1.2547],\n",
      "        [ 0.1790, -0.0719],\n",
      "        [-0.0635, -1.0997],\n",
      "        [-1.8621, -0.8992],\n",
      "        [-0.2615, -0.9002],\n",
      "        [ 1.1677, -0.3271],\n",
      "        [-1.5209,  1.2732],\n",
      "        [ 1.6287, -1.5534],\n",
      "        [ 0.1487,  0.6603]]) tensor([ 6.7273,  9.9036,  4.8003,  7.8078,  3.5347,  6.7192,  7.6470, -3.1736,\n",
      "        12.7450,  2.2450])\n",
      "tensor([[-1.5291,  0.9193],\n",
      "        [ 0.2676,  1.4034],\n",
      "        [ 0.4689, -0.2217],\n",
      "        [ 1.6151,  2.3568],\n",
      "        [ 0.6388, -0.6430],\n",
      "        [-0.2921,  0.0468],\n",
      "        [ 0.7216,  0.1682],\n",
      "        [-0.4320,  0.5557],\n",
      "        [-0.1064, -2.1063],\n",
      "        [-0.6491, -0.4650]]) tensor([-1.9896, -0.0240,  5.9010, -0.5904,  7.6588,  3.4429,  5.0630,  1.4473,\n",
      "        11.1264,  4.4694])\n",
      "tensor([[ 0.7540, -0.2171],\n",
      "        [-0.6358, -1.8260],\n",
      "        [-0.1466,  0.0186],\n",
      "        [ 0.2641, -1.1446],\n",
      "        [-0.5617,  0.9681],\n",
      "        [ 0.6334, -0.7401],\n",
      "        [-0.7372, -0.7344],\n",
      "        [ 1.0277, -0.7192],\n",
      "        [ 0.1357,  0.9720],\n",
      "        [-1.2778, -0.8719]]) tensor([ 6.4613,  9.1418,  3.8555,  8.6230, -0.2017,  7.9828,  5.2177,  8.6853,\n",
      "         1.1601,  4.5982])\n",
      "tensor([[ 0.5002,  0.9741],\n",
      "        [ 1.5649,  0.1864],\n",
      "        [-1.3476, -2.3845],\n",
      "        [-1.4723, -0.2179],\n",
      "        [ 1.5260,  0.7267],\n",
      "        [-0.5692, -2.0024],\n",
      "        [ 0.8373,  0.8791],\n",
      "        [-0.0831,  2.4963],\n",
      "        [ 0.5947, -0.3830],\n",
      "        [-1.3745,  1.4810]]) tensor([ 1.8904,  6.6972,  9.6130,  1.9954,  4.8004,  9.8708,  2.8792, -4.4504,\n",
      "         6.7136, -3.5922])\n",
      "tensor([[ 0.5102,  0.1333],\n",
      "        [ 0.5009,  0.3930],\n",
      "        [-0.9111,  1.2799],\n",
      "        [ 1.0546, -1.0462],\n",
      "        [ 0.6213, -0.3449],\n",
      "        [-0.4651,  1.3077],\n",
      "        [-0.3749, -0.5381],\n",
      "        [-1.6673, -1.0350],\n",
      "        [ 0.1340,  1.6294],\n",
      "        [ 0.1273, -0.1458]]) tensor([ 4.7828,  3.8439, -1.9781,  9.8558,  6.6353, -1.1810,  5.2856,  4.3826,\n",
      "        -1.0616,  4.9546])\n",
      "tensor([[ 0.5112,  1.2719],\n",
      "        [ 0.9667, -0.2397],\n",
      "        [-2.3675, -1.0868],\n",
      "        [-1.2473,  1.0521],\n",
      "        [-0.3736,  1.4061],\n",
      "        [ 1.6853, -1.6282],\n",
      "        [ 0.6321,  1.5009],\n",
      "        [ 0.3441,  0.1984],\n",
      "        [-0.5583, -1.1814],\n",
      "        [-0.5627, -0.1611]]) tensor([ 0.9062,  6.9664,  3.1602, -1.8728, -1.3306, 13.1179,  0.3555,  4.2053,\n",
      "         7.1161,  3.6291])\n",
      "tensor([[-0.8655,  1.5021],\n",
      "        [-1.8493,  0.3891],\n",
      "        [ 1.2030,  1.4122],\n",
      "        [-1.0800, -0.5036],\n",
      "        [ 0.0119,  1.6596],\n",
      "        [-0.5856,  1.9091],\n",
      "        [ 1.5818,  0.7987],\n",
      "        [ 0.9343, -1.6526],\n",
      "        [-0.7177,  0.3329],\n",
      "        [ 1.0365, -0.2421]]) tensor([-2.6271, -0.8108,  1.8249,  3.7351, -1.4236, -3.4650,  4.6344, 11.6853,\n",
      "         1.6319,  7.0935])\n",
      "tensor([[ 0.4667,  0.0442],\n",
      "        [-0.4802, -0.8634],\n",
      "        [-0.9240, -1.7888],\n",
      "        [-0.2962,  0.7328],\n",
      "        [ 0.3432, -1.2304],\n",
      "        [ 0.0915, -0.1224],\n",
      "        [-0.7179, -0.5703],\n",
      "        [ 0.3698,  1.2541],\n",
      "        [ 1.2515, -2.0151],\n",
      "        [-1.3206,  0.4077]]) tensor([ 4.9694,  6.1842,  8.4239,  1.1214,  9.0775,  4.7908,  4.7070,  0.6852,\n",
      "        13.5598,  0.1743])\n",
      "tensor([[ 2.3612,  1.0230],\n",
      "        [-0.6272,  0.0114],\n",
      "        [-0.3208, -0.3617],\n",
      "        [-0.9992, -0.1596],\n",
      "        [ 1.6990, -1.5076],\n",
      "        [-0.3889,  1.9604],\n",
      "        [-0.4084, -0.5699],\n",
      "        [-1.4201, -0.5744],\n",
      "        [-1.1049, -0.9207],\n",
      "        [ 1.2262, -0.7417]]) tensor([ 5.4418,  2.9136,  4.7795,  2.7489, 12.7136, -3.2617,  5.3161,  3.3208,\n",
      "         5.1169,  9.1632])\n",
      "tensor([[ 1.3626,  1.5224],\n",
      "        [-0.0588, -0.3937],\n",
      "        [-0.3569,  0.9907],\n",
      "        [-0.9945,  1.3048],\n",
      "        [ 0.1377,  0.3380],\n",
      "        [ 0.7295, -1.2616],\n",
      "        [ 1.2653,  0.0141],\n",
      "        [ 0.0336, -0.1103],\n",
      "        [-0.0180, -0.1912],\n",
      "        [ 0.5295,  0.8823]]) tensor([ 1.7576,  5.4084,  0.1293, -2.2272,  3.3331,  9.9400,  6.6836,  4.6477,\n",
      "         4.7974,  2.2618])\n",
      "tensor([[-0.3483,  1.6251],\n",
      "        [ 0.7292, -0.7145],\n",
      "        [-0.4456, -0.5476],\n",
      "        [ 0.9010,  0.7562],\n",
      "        [ 1.3696,  0.9708],\n",
      "        [-0.1347,  1.4003],\n",
      "        [-0.6830,  0.2023],\n",
      "        [ 0.7988, -1.5289],\n",
      "        [ 2.0287,  0.6398],\n",
      "        [ 0.4838,  0.7574]]) tensor([-2.0350,  8.1091,  5.1725,  3.4258,  3.6331, -0.8171,  2.1449, 10.9948,\n",
      "         6.0760,  2.5852])\n",
      "tensor([[-0.2947, -0.3435],\n",
      "        [-0.2330, -2.5376],\n",
      "        [-1.4671, -0.8093],\n",
      "        [-0.7096,  0.7509],\n",
      "        [ 1.5573, -0.5529],\n",
      "        [ 0.2497, -0.1361],\n",
      "        [-0.0403,  0.1485],\n",
      "        [ 0.0467,  0.3073],\n",
      "        [-0.4198, -0.8204],\n",
      "        [ 0.3460, -0.1636]]) tensor([ 4.7658, 12.3701,  4.0259,  0.2309,  9.2094,  5.1434,  3.5985,  3.2438,\n",
      "         6.1490,  5.4541])\n",
      "tensor([[-0.6216, -0.5573],\n",
      "        [ 0.1732, -0.0663],\n",
      "        [ 1.0217,  0.6474],\n",
      "        [-0.6190, -2.9205],\n",
      "        [-0.1039,  1.0072],\n",
      "        [ 0.3278, -2.1964],\n",
      "        [-1.1871,  1.0074],\n",
      "        [-1.1403,  0.7801],\n",
      "        [-1.3375, -1.5696],\n",
      "        [ 2.9581, -1.6161]]) tensor([ 4.8419,  4.7663,  4.0529, 12.8993,  0.5464, 12.3309, -1.5966, -0.7351,\n",
      "         6.8496, 15.5991])\n",
      "tensor([[-0.5517,  0.5802],\n",
      "        [-0.6525,  0.6747],\n",
      "        [ 0.9109, -1.5196],\n",
      "        [-0.9942, -1.5738],\n",
      "        [-0.9978, -0.4297],\n",
      "        [ 1.0986,  1.6135],\n",
      "        [ 0.8213, -0.1594],\n",
      "        [ 1.1714, -1.1294],\n",
      "        [-1.1933, -1.4006],\n",
      "        [-0.6139,  0.1480]]) tensor([ 1.1314,  0.5974, 11.2044,  7.5629,  3.6653,  0.8954,  6.3878, 10.3857,\n",
      "         6.5959,  2.4754])\n",
      "tensor([[ 0.3847, -1.1388],\n",
      "        [-0.9767,  0.6784],\n",
      "        [-0.5133,  1.2148],\n",
      "        [-0.1049,  1.6535],\n",
      "        [-2.0101,  0.5397],\n",
      "        [ 1.1793, -0.0123],\n",
      "        [-0.3226, -0.9060],\n",
      "        [-0.4819,  0.5463],\n",
      "        [-0.3418,  0.5562],\n",
      "        [ 0.1437, -0.0562]]) tensor([ 8.8284, -0.0595, -0.9577, -1.6343, -1.6469,  6.6036,  6.6320,  1.3770,\n",
      "         1.6299,  4.6870])\n",
      "tensor([[ 0.6495, -0.3806],\n",
      "        [ 0.7915, -0.0385],\n",
      "        [ 0.1411,  0.4599],\n",
      "        [ 1.6569, -1.2354],\n",
      "        [ 0.5977, -0.3457],\n",
      "        [ 1.1669,  0.0860],\n",
      "        [ 1.5216, -1.6789],\n",
      "        [ 1.6446, -0.3193],\n",
      "        [ 0.1613, -0.7903],\n",
      "        [ 0.6332, -1.8363]]) tensor([ 6.8027,  5.9166,  2.9126, 11.7266,  6.5714,  6.2453, 12.9471,  8.5709,\n",
      "         7.2172, 11.7201])\n",
      "tensor([[ 0.7285, -0.8230],\n",
      "        [ 0.6937, -1.7669],\n",
      "        [ 0.8564, -1.1543],\n",
      "        [ 1.5971,  1.1298],\n",
      "        [ 1.5173,  1.7886],\n",
      "        [-0.1204, -1.6792],\n",
      "        [-0.5744,  0.0617],\n",
      "        [-1.1081, -0.9717],\n",
      "        [-0.9021,  0.0798],\n",
      "        [-0.6186, -0.8054]]) tensor([ 8.4667, 11.5926,  9.8252,  3.5608,  1.1503,  9.6513,  2.8612,  5.2883,\n",
      "         2.1352,  5.6915])\n",
      "tensor([[ 1.6542, -1.7830],\n",
      "        [-1.4436, -0.6686],\n",
      "        [-0.6819, -1.3983],\n",
      "        [ 0.4341,  0.4592],\n",
      "        [ 1.6957, -1.5593],\n",
      "        [ 0.7510,  0.6632],\n",
      "        [ 0.9389,  1.6844],\n",
      "        [-1.1711,  0.6492],\n",
      "        [ 0.6474, -0.5324],\n",
      "        [-0.4466, -1.1614]]) tensor([13.5749,  3.5909,  7.5950,  3.5045, 12.8879,  3.4546,  0.3404, -0.3665,\n",
      "         7.3082,  7.2536])\n",
      "tensor([[ 0.6755,  2.3713],\n",
      "        [ 0.9982,  0.3214],\n",
      "        [ 1.4087,  0.3379],\n",
      "        [-0.4984,  1.8576],\n",
      "        [-0.1456,  0.2317],\n",
      "        [ 0.5276, -0.2378],\n",
      "        [ 0.8683, -0.4335],\n",
      "        [-0.8158, -0.1849],\n",
      "        [ 1.0008, -0.5285],\n",
      "        [ 0.4282, -0.5139]]) tensor([-2.4998,  5.0987,  5.8720, -3.1026,  3.1261,  6.0662,  7.4183,  3.2116,\n",
      "         8.0061,  6.8009])\n",
      "tensor([[ 0.7588, -0.3985],\n",
      "        [-0.6793, -0.8531],\n",
      "        [ 0.2519, -0.8684],\n",
      "        [ 0.6452, -1.5642],\n",
      "        [ 0.2448,  0.5360],\n",
      "        [-1.2958,  0.4991],\n",
      "        [ 1.0692, -0.9871],\n",
      "        [ 0.1774, -1.4504],\n",
      "        [-0.9850, -0.4228],\n",
      "        [-2.0365, -0.7985]]) tensor([ 7.0719,  5.7467,  7.6350, 10.8183,  2.8615, -0.0905,  9.6936,  9.4737,\n",
      "         3.6725,  2.8479])\n",
      "tensor([[-2.0103, -0.9581],\n",
      "        [-0.6590, -1.5227],\n",
      "        [-1.8057,  0.4311],\n",
      "        [-0.2739,  1.6957],\n",
      "        [-1.0955,  1.2518],\n",
      "        [ 1.5605, -0.1425],\n",
      "        [-0.4540, -1.4679],\n",
      "        [-0.7198, -0.1399],\n",
      "        [-1.1607, -3.0372],\n",
      "        [ 0.6383,  0.1559]]) tensor([ 3.4438,  8.0731, -0.8803, -2.1048, -2.2546,  7.8033,  8.2647,  3.2282,\n",
      "        12.2114,  4.9562])\n",
      "tensor([[ 0.1479, -0.2148],\n",
      "        [ 0.5323, -0.7561],\n",
      "        [ 0.9350, -0.1074],\n",
      "        [-0.7210, -0.6488],\n",
      "        [ 0.4247,  0.4367],\n",
      "        [ 1.6467,  0.8451],\n",
      "        [ 0.8398,  0.2322],\n",
      "        [-0.2505, -0.1841],\n",
      "        [ 0.1792,  1.5887],\n",
      "        [ 1.6525,  0.5723]]) tensor([ 5.2265,  7.8543,  6.4325,  4.9613,  3.5685,  4.6171,  5.1043,  4.3349,\n",
      "        -0.8440,  5.5626])\n",
      "tensor([[-0.1602, -0.9110],\n",
      "        [ 0.1628,  0.9368],\n",
      "        [-0.1379, -1.5225],\n",
      "        [-0.1992,  0.7261],\n",
      "        [ 1.8097,  1.7872],\n",
      "        [ 0.3720, -2.4216],\n",
      "        [ 0.2866, -0.5749],\n",
      "        [ 0.4701,  0.2231],\n",
      "        [-0.5070, -0.4492],\n",
      "        [ 0.2046,  1.5247]]) tensor([ 6.9784,  1.3573,  9.0877,  1.3388,  1.7311, 13.1717,  6.7234,  4.3719,\n",
      "         4.7095, -0.5725])\n",
      "tensor([[ 0.9897, -1.9962],\n",
      "        [-1.6979,  1.0972],\n",
      "        [-0.2445,  0.3261],\n",
      "        [-1.0252,  0.8441],\n",
      "        [ 0.2343,  0.3983],\n",
      "        [ 0.4398, -1.7473],\n",
      "        [-0.9043, -2.4972],\n",
      "        [ 0.0790, -1.0673],\n",
      "        [ 1.0320, -0.9692],\n",
      "        [ 1.2083, -0.6057]]) tensor([12.9557, -2.9148,  2.6130, -0.7160,  3.3251, 11.0330, 10.8826,  7.9842,\n",
      "         9.5689,  8.6717])\n",
      "tensor([[ 0.0776, -0.1657],\n",
      "        [ 0.5384, -1.0827],\n",
      "        [ 1.5135, -1.2910],\n",
      "        [-0.1789,  0.7317],\n",
      "        [-0.1703, -0.2000],\n",
      "        [ 0.6388, -0.1763],\n",
      "        [-1.8871,  0.1114],\n",
      "        [-0.2289,  0.6642],\n",
      "        [-1.7404,  0.2389],\n",
      "        [-1.9538,  1.1694]]) tensor([ 4.9154,  8.9601, 11.6093,  1.3512,  4.5359,  6.0836,  0.0301,  1.4778,\n",
      "        -0.1062, -3.6770])\n",
      "tensor([[-0.8059, -0.6631],\n",
      "        [-0.4382, -0.0327],\n",
      "        [-0.5374, -0.0384],\n",
      "        [ 0.0315, -0.5442],\n",
      "        [ 1.5026,  2.6455],\n",
      "        [-0.8556,  0.9795],\n",
      "        [ 0.1639, -0.0381],\n",
      "        [-1.7144,  2.2632],\n",
      "        [-0.8926,  0.2472],\n",
      "        [-0.6070,  0.5198]]) tensor([ 4.8492,  3.4441,  3.2502,  6.1071, -1.7961, -0.8512,  4.6528, -6.9126,\n",
      "         1.5835,  1.2031])\n",
      "tensor([[-1.3788,  1.7600],\n",
      "        [-1.7074,  0.2235],\n",
      "        [ 0.8024,  0.1013],\n",
      "        [ 0.8401,  1.4700],\n",
      "        [-0.0921, -0.3523],\n",
      "        [ 0.8088,  1.7579],\n",
      "        [ 0.5126, -0.4142],\n",
      "        [ 0.0813, -0.7251],\n",
      "        [ 0.0269, -0.6952],\n",
      "        [-0.6816,  0.1869]]) tensor([-4.5366,  0.0094,  5.4663,  0.8729,  5.2108, -0.1688,  6.6270,  6.8453,\n",
      "         6.6091,  2.2098])\n",
      "tensor([[ 1.5501, -0.6383],\n",
      "        [ 0.3334,  1.5918],\n",
      "        [-0.2845,  0.9865],\n",
      "        [-0.6848, -1.3891],\n",
      "        [ 0.5353,  0.4805],\n",
      "        [-0.8991, -0.8978],\n",
      "        [ 0.9555, -0.6944],\n",
      "        [-1.0828, -0.6708],\n",
      "        [ 0.1595,  0.5879],\n",
      "        [-0.0764,  0.0388]]) tensor([ 9.4844, -0.5528,  0.2860,  7.5612,  3.6171,  5.4572,  8.4637,  4.3269,\n",
      "         2.5258,  3.9192])\n",
      "tensor([[ 0.1241, -0.0769],\n",
      "        [ 0.0290,  0.9020],\n",
      "        [ 0.2335, -0.1615],\n",
      "        [-0.6944, -0.7395],\n",
      "        [ 1.9644, -0.7339],\n",
      "        [ 0.0382, -0.7558],\n",
      "        [ 0.8631, -0.9609],\n",
      "        [ 0.9122,  0.2905],\n",
      "        [ 0.2559,  0.9517],\n",
      "        [-0.5232, -0.8360]]) tensor([ 4.7060,  1.1927,  5.2273,  5.3193, 10.6133,  6.8389,  9.1912,  5.0376,\n",
      "         1.4801,  6.0244])\n",
      "tensor([[-0.6832,  2.3971],\n",
      "        [-1.2518,  0.3666],\n",
      "        [ 2.3062,  0.6838],\n",
      "        [ 1.1903,  0.5455],\n",
      "        [ 1.6635, -1.7517],\n",
      "        [ 1.9343,  1.7904],\n",
      "        [-0.8043, -0.3960],\n",
      "        [ 0.6192,  0.5581],\n",
      "        [-1.4157, -0.6877],\n",
      "        [-0.6163, -1.2211]]) tensor([-5.3086,  0.4432,  6.4834,  4.7242, 13.4667,  1.9679,  3.9250,  3.5437,\n",
      "         3.7079,  7.1172])\n",
      "tensor([[-0.2908,  0.2499],\n",
      "        [ 0.4082,  0.7605],\n",
      "        [ 1.1813, -1.2298],\n",
      "        [ 0.6872,  1.0711],\n",
      "        [ 1.4857,  0.5264],\n",
      "        [-0.6912, -0.0868],\n",
      "        [-0.6020, -0.3380],\n",
      "        [-0.6689, -0.4812],\n",
      "        [ 0.5193,  0.3495],\n",
      "        [-0.9955, -0.3030]]) tensor([ 2.7817,  2.4439, 10.7423,  1.9410,  5.3769,  3.0956,  4.1456,  4.5180,\n",
      "         4.0464,  3.2352])\n",
      "tensor([[ 1.3700,  0.9631],\n",
      "        [ 1.2904,  1.1000],\n",
      "        [ 0.6774, -0.6746],\n",
      "        [ 0.7742,  0.5063],\n",
      "        [ 0.0230,  2.3636],\n",
      "        [-0.4501, -0.7899],\n",
      "        [ 0.4163, -1.7354],\n",
      "        [-0.2342,  1.2494],\n",
      "        [-0.5137, -0.6979],\n",
      "        [ 0.6623, -0.8651]]) tensor([ 3.6442,  3.0350,  7.8299,  4.0165, -3.7830,  5.9747, 10.9386, -0.5082,\n",
      "         5.5496,  8.4704])\n",
      "tensor([[ 0.6433, -0.0117],\n",
      "        [-1.9505, -0.4903],\n",
      "        [ 0.3148, -1.4887],\n",
      "        [-0.1221,  1.9698],\n",
      "        [ 0.1445, -0.5255],\n",
      "        [ 1.1801, -0.9593],\n",
      "        [ 0.8544,  0.3049],\n",
      "        [-1.2801,  0.7067],\n",
      "        [ 0.1456, -0.6606],\n",
      "        [-0.8446, -1.3381]]) tensor([ 5.4976,  1.9580,  9.8765, -2.7441,  6.2773,  9.8037,  4.8805, -0.7647,\n",
      "         6.7514,  7.0569])\n",
      "tensor([[ 0.0505,  1.8145],\n",
      "        [ 0.5375,  0.1649],\n",
      "        [ 0.3039, -1.2600],\n",
      "        [-0.5241, -1.2737],\n",
      "        [-0.3915,  0.8109],\n",
      "        [-1.6758,  0.4081],\n",
      "        [ 0.6366,  1.8263],\n",
      "        [-1.3819, -0.4450],\n",
      "        [-2.1324, -0.6836],\n",
      "        [ 0.6545, -2.0414]]) tensor([-1.8780,  4.6984,  9.1052,  7.4901,  0.6569, -0.5402, -0.7149,  2.9457,\n",
      "         2.2658, 12.4528])\n",
      "tensor([[ 0.5143, -0.5133],\n",
      "        [-0.6694, -0.3245],\n",
      "        [ 0.8898,  0.8849],\n",
      "        [-0.3707, -2.4818],\n",
      "        [ 0.6304, -0.7779],\n",
      "        [-0.1513, -1.9662],\n",
      "        [ 0.7477,  0.4046],\n",
      "        [ 0.6386,  0.1517],\n",
      "        [ 0.6611,  0.0193],\n",
      "        [ 0.1130,  0.1397]]) tensor([ 6.9799,  3.9681,  2.9754, 11.8932,  8.1171, 10.5875,  4.3313,  4.9528,\n",
      "         5.4554,  3.9468])\n",
      "tensor([[ 0.6422, -0.7585],\n",
      "        [-0.0401, -1.6241],\n",
      "        [-0.0357,  0.4525],\n",
      "        [ 1.8247, -0.1633],\n",
      "        [ 2.2137, -0.1507],\n",
      "        [ 1.1233,  1.8247],\n",
      "        [ 0.4608,  0.5876],\n",
      "        [-0.6832, -0.5971],\n",
      "        [ 1.7439, -0.6361],\n",
      "        [ 0.9383,  0.7238]]) tensor([8.0821, 9.6488, 2.5880, 8.4191, 9.1252, 0.2426, 3.1128, 4.8663, 9.8496,\n",
      "        3.6218])\n",
      "tensor([[ 1.7859, -0.1711],\n",
      "        [-1.0564, -1.6183],\n",
      "        [-1.0838, -0.5149],\n",
      "        [-1.5382, -0.2056],\n",
      "        [ 1.1265, -0.9374],\n",
      "        [-1.6479, -0.5099],\n",
      "        [-0.0786,  0.0223],\n",
      "        [-0.2385, -0.4470],\n",
      "        [ 0.5692, -0.8192],\n",
      "        [-0.2570, -0.4582]]) tensor([8.3518, 7.5959, 3.7932, 1.8240, 9.6330, 2.6396, 3.9690, 5.2502, 8.1287,\n",
      "        5.2488])\n",
      "tensor([[-0.5317, -0.5385],\n",
      "        [-2.4695, -0.9225],\n",
      "        [ 0.3050,  1.6013],\n",
      "        [-1.4794, -1.1723],\n",
      "        [ 0.1366, -1.0630],\n",
      "        [-1.5003,  0.4431],\n",
      "        [-0.2957,  0.0804],\n",
      "        [ 0.2523,  0.3845],\n",
      "        [-1.4974, -0.2904],\n",
      "        [-0.3197,  0.6775]]) tensor([ 4.9759,  2.3994, -0.6340,  5.2355,  8.0809, -0.2940,  3.3531,  3.4111,\n",
      "         2.1787,  1.2558])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "PyTorch提供了大量预定义的层，这使我们只需关注使用哪些层来构造模型。下面将介绍如何使用PyTorch更简洁地定义线性回归。\n",
    "首先导入`torch.nn`模块。之前我们使用了`autograd`这里`nn`就是使用了`autograd`来定义模型。`nn`的核心数据结构是`Module`，它是一个抽象概念，可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承`nn.Module`，撰写自己的网络/层。一个`nn.Module`实例应该包含一些层以及返回输出的前向传播(forward)方法。下面先来看看如何用`nn.Module`实现一个线性回归模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(n_feature, 1)\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "          ('linear', nn.Linear(num_inputs, 1))\n",
    "          # ......\n",
    "        ]))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过`net.parameters()`来查看模型所有的可学习参数，此函数将返回一个生成器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3620,  0.1003]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0771], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "在使用`net`前，我们需要初始化模型参数，如线性回归模型中的权重和偏差。PyTorch在`init`模块中提供了多种参数初始化方法。通过`init.normal_`将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差初始化为0。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.normal_(net[0].weight, mean=0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "我们采用PyTorch的均方误差损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化算法\n",
    "我们无须自己实现小批量随机梯度下降算法。`torch.optim`模块提供了很多常用的优化算法比如SGD、Adam等。下面我们创建一个用于优化`net`所有参数的优化器实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr = 0.001)\n",
    "print(optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000073\n",
      "epoch 2, loss: 0.000061\n",
      "epoch 3, loss: 0.000116\n",
      "epoch 4, loss: 0.000048\n",
      "epoch 5, loss: 0.000097\n",
      "epoch 6, loss: 0.000132\n",
      "epoch 7, loss: 0.000080\n",
      "epoch 8, loss: 0.000076\n",
      "epoch 9, loss: 0.000150\n",
      "epoch 10, loss: 0.000183\n",
      "epoch 11, loss: 0.000051\n",
      "epoch 12, loss: 0.000088\n",
      "epoch 13, loss: 0.000079\n",
      "epoch 14, loss: 0.000239\n",
      "epoch 15, loss: 0.000111\n",
      "epoch 16, loss: 0.000051\n",
      "epoch 17, loss: 0.000084\n",
      "epoch 18, loss: 0.000116\n",
      "epoch 19, loss: 0.000061\n",
      "epoch 20, loss: 0.000138\n",
      "epoch 21, loss: 0.000121\n",
      "epoch 22, loss: 0.000087\n",
      "epoch 23, loss: 0.000148\n",
      "epoch 24, loss: 0.000076\n",
      "epoch 25, loss: 0.000192\n",
      "epoch 26, loss: 0.000148\n",
      "epoch 27, loss: 0.000172\n",
      "epoch 28, loss: 0.000078\n",
      "epoch 29, loss: 0.000036\n",
      "epoch 30, loss: 0.000098\n",
      "epoch 31, loss: 0.000110\n",
      "epoch 32, loss: 0.000062\n",
      "epoch 33, loss: 0.000072\n",
      "epoch 34, loss: 0.000128\n",
      "epoch 35, loss: 0.000127\n",
      "epoch 36, loss: 0.000106\n",
      "epoch 37, loss: 0.000239\n",
      "epoch 38, loss: 0.000050\n",
      "epoch 39, loss: 0.000158\n",
      "epoch 40, loss: 0.000224\n",
      "epoch 41, loss: 0.000036\n",
      "epoch 42, loss: 0.000102\n",
      "epoch 43, loss: 0.000103\n",
      "epoch 44, loss: 0.000091\n",
      "epoch 45, loss: 0.000136\n",
      "epoch 46, loss: 0.000096\n",
      "epoch 47, loss: 0.000077\n",
      "epoch 48, loss: 0.000116\n",
      "epoch 49, loss: 0.000129\n",
      "epoch 50, loss: 0.000173\n",
      "epoch 51, loss: 0.000158\n",
      "epoch 52, loss: 0.000082\n",
      "epoch 53, loss: 0.000063\n",
      "epoch 54, loss: 0.000067\n",
      "epoch 55, loss: 0.000174\n",
      "epoch 56, loss: 0.000101\n",
      "epoch 57, loss: 0.000066\n",
      "epoch 58, loss: 0.000138\n",
      "epoch 59, loss: 0.000061\n",
      "epoch 60, loss: 0.000063\n",
      "epoch 61, loss: 0.000106\n",
      "epoch 62, loss: 0.000111\n",
      "epoch 63, loss: 0.000149\n",
      "epoch 64, loss: 0.000081\n",
      "epoch 65, loss: 0.000096\n",
      "epoch 66, loss: 0.000105\n",
      "epoch 67, loss: 0.000068\n",
      "epoch 68, loss: 0.000114\n",
      "epoch 69, loss: 0.000154\n",
      "epoch 70, loss: 0.000128\n",
      "epoch 71, loss: 0.000095\n",
      "epoch 72, loss: 0.000117\n",
      "epoch 73, loss: 0.000051\n",
      "epoch 74, loss: 0.000115\n",
      "epoch 75, loss: 0.000102\n",
      "epoch 76, loss: 0.000055\n",
      "epoch 77, loss: 0.000119\n",
      "epoch 78, loss: 0.000083\n",
      "epoch 79, loss: 0.000057\n",
      "epoch 80, loss: 0.000095\n",
      "epoch 81, loss: 0.000076\n",
      "epoch 82, loss: 0.000123\n",
      "epoch 83, loss: 0.000050\n",
      "epoch 84, loss: 0.000097\n",
      "epoch 85, loss: 0.000067\n",
      "epoch 86, loss: 0.000083\n",
      "epoch 87, loss: 0.000175\n",
      "epoch 88, loss: 0.000090\n",
      "epoch 89, loss: 0.000145\n",
      "epoch 90, loss: 0.000083\n",
      "epoch 91, loss: 0.000079\n",
      "epoch 92, loss: 0.000132\n",
      "epoch 93, loss: 0.000050\n",
      "epoch 94, loss: 0.000115\n",
      "epoch 95, loss: 0.000114\n",
      "epoch 96, loss: 0.000142\n",
      "epoch 97, loss: 0.000053\n",
      "epoch 98, loss: 0.000112\n",
      "epoch 99, loss: 0.000043\n",
      "epoch 100, loss: 0.000046\n",
      "epoch 101, loss: 0.000065\n",
      "epoch 102, loss: 0.000048\n",
      "epoch 103, loss: 0.000075\n",
      "epoch 104, loss: 0.000106\n",
      "epoch 105, loss: 0.000057\n",
      "epoch 106, loss: 0.000118\n",
      "epoch 107, loss: 0.000051\n",
      "epoch 108, loss: 0.000093\n",
      "epoch 109, loss: 0.000149\n",
      "epoch 110, loss: 0.000144\n",
      "epoch 111, loss: 0.000147\n",
      "epoch 112, loss: 0.000084\n",
      "epoch 113, loss: 0.000138\n",
      "epoch 114, loss: 0.000069\n",
      "epoch 115, loss: 0.000129\n",
      "epoch 116, loss: 0.000070\n",
      "epoch 117, loss: 0.000110\n",
      "epoch 118, loss: 0.000123\n",
      "epoch 119, loss: 0.000061\n",
      "epoch 120, loss: 0.000060\n",
      "epoch 121, loss: 0.000091\n",
      "epoch 122, loss: 0.000046\n",
      "epoch 123, loss: 0.000118\n",
      "epoch 124, loss: 0.000142\n",
      "epoch 125, loss: 0.000183\n",
      "epoch 126, loss: 0.000106\n",
      "epoch 127, loss: 0.000037\n",
      "epoch 128, loss: 0.000046\n",
      "epoch 129, loss: 0.000134\n",
      "epoch 130, loss: 0.000093\n",
      "epoch 131, loss: 0.000105\n",
      "epoch 132, loss: 0.000172\n",
      "epoch 133, loss: 0.000149\n",
      "epoch 134, loss: 0.000064\n",
      "epoch 135, loss: 0.000159\n",
      "epoch 136, loss: 0.000136\n",
      "epoch 137, loss: 0.000033\n",
      "epoch 138, loss: 0.000053\n",
      "epoch 139, loss: 0.000212\n",
      "epoch 140, loss: 0.000193\n",
      "epoch 141, loss: 0.000037\n",
      "epoch 142, loss: 0.000111\n",
      "epoch 143, loss: 0.000166\n",
      "epoch 144, loss: 0.000082\n",
      "epoch 145, loss: 0.000047\n",
      "epoch 146, loss: 0.000067\n",
      "epoch 147, loss: 0.000042\n",
      "epoch 148, loss: 0.000022\n",
      "epoch 149, loss: 0.000135\n",
      "epoch 150, loss: 0.000083\n",
      "epoch 151, loss: 0.000142\n",
      "epoch 152, loss: 0.000041\n",
      "epoch 153, loss: 0.000152\n",
      "epoch 154, loss: 0.000105\n",
      "epoch 155, loss: 0.000071\n",
      "epoch 156, loss: 0.000086\n",
      "epoch 157, loss: 0.000081\n",
      "epoch 158, loss: 0.000088\n",
      "epoch 159, loss: 0.000066\n",
      "epoch 160, loss: 0.000091\n",
      "epoch 161, loss: 0.000155\n",
      "epoch 162, loss: 0.000057\n",
      "epoch 163, loss: 0.000132\n",
      "epoch 164, loss: 0.000048\n",
      "epoch 165, loss: 0.000151\n",
      "epoch 166, loss: 0.000036\n",
      "epoch 167, loss: 0.000075\n",
      "epoch 168, loss: 0.000172\n",
      "epoch 169, loss: 0.000063\n",
      "epoch 170, loss: 0.000050\n",
      "epoch 171, loss: 0.000079\n",
      "epoch 172, loss: 0.000142\n",
      "epoch 173, loss: 0.000108\n",
      "epoch 174, loss: 0.000052\n",
      "epoch 175, loss: 0.000185\n",
      "epoch 176, loss: 0.000111\n",
      "epoch 177, loss: 0.000193\n",
      "epoch 178, loss: 0.000063\n",
      "epoch 179, loss: 0.000147\n",
      "epoch 180, loss: 0.000052\n",
      "epoch 181, loss: 0.000141\n",
      "epoch 182, loss: 0.000129\n",
      "epoch 183, loss: 0.000059\n",
      "epoch 184, loss: 0.000082\n",
      "epoch 185, loss: 0.000077\n",
      "epoch 186, loss: 0.000104\n",
      "epoch 187, loss: 0.000130\n",
      "epoch 188, loss: 0.000212\n",
      "epoch 189, loss: 0.000155\n",
      "epoch 190, loss: 0.000194\n",
      "epoch 191, loss: 0.000067\n",
      "epoch 192, loss: 0.000070\n",
      "epoch 193, loss: 0.000102\n",
      "epoch 194, loss: 0.000095\n",
      "epoch 195, loss: 0.000049\n",
      "epoch 196, loss: 0.000076\n",
      "epoch 197, loss: 0.000071\n",
      "epoch 198, loss: 0.000133\n",
      "epoch 199, loss: 0.000081\n",
      "epoch 200, loss: 0.000152\n",
      "epoch 201, loss: 0.000106\n",
      "epoch 202, loss: 0.000068\n",
      "epoch 203, loss: 0.000099\n",
      "epoch 204, loss: 0.000141\n",
      "epoch 205, loss: 0.000132\n",
      "epoch 206, loss: 0.000069\n",
      "epoch 207, loss: 0.000124\n",
      "epoch 208, loss: 0.000189\n",
      "epoch 209, loss: 0.000069\n",
      "epoch 210, loss: 0.000020\n",
      "epoch 211, loss: 0.000169\n",
      "epoch 212, loss: 0.000082\n",
      "epoch 213, loss: 0.000137\n",
      "epoch 214, loss: 0.000064\n",
      "epoch 215, loss: 0.000107\n",
      "epoch 216, loss: 0.000172\n",
      "epoch 217, loss: 0.000065\n",
      "epoch 218, loss: 0.000048\n",
      "epoch 219, loss: 0.000079\n",
      "epoch 220, loss: 0.000074\n",
      "epoch 221, loss: 0.000113\n",
      "epoch 222, loss: 0.000147\n",
      "epoch 223, loss: 0.000061\n",
      "epoch 224, loss: 0.000073\n",
      "epoch 225, loss: 0.000069\n",
      "epoch 226, loss: 0.000256\n",
      "epoch 227, loss: 0.000164\n",
      "epoch 228, loss: 0.000146\n",
      "epoch 229, loss: 0.000063\n",
      "epoch 230, loss: 0.000097\n",
      "epoch 231, loss: 0.000074\n",
      "epoch 232, loss: 0.000082\n",
      "epoch 233, loss: 0.000089\n",
      "epoch 234, loss: 0.000084\n",
      "epoch 235, loss: 0.000077\n",
      "epoch 236, loss: 0.000082\n",
      "epoch 237, loss: 0.000051\n",
      "epoch 238, loss: 0.000080\n",
      "epoch 239, loss: 0.000086\n",
      "epoch 240, loss: 0.000165\n",
      "epoch 241, loss: 0.000027\n",
      "epoch 242, loss: 0.000091\n",
      "epoch 243, loss: 0.000060\n",
      "epoch 244, loss: 0.000039\n",
      "epoch 245, loss: 0.000145\n",
      "epoch 246, loss: 0.000053\n",
      "epoch 247, loss: 0.000127\n",
      "epoch 248, loss: 0.000065\n",
      "epoch 249, loss: 0.000080\n",
      "epoch 250, loss: 0.000224\n",
      "epoch 251, loss: 0.000172\n",
      "epoch 252, loss: 0.000084\n",
      "epoch 253, loss: 0.000051\n",
      "epoch 254, loss: 0.000082\n",
      "epoch 255, loss: 0.000082\n",
      "epoch 256, loss: 0.000068\n",
      "epoch 257, loss: 0.000130\n",
      "epoch 258, loss: 0.000157\n",
      "epoch 259, loss: 0.000214\n",
      "epoch 260, loss: 0.000160\n",
      "epoch 261, loss: 0.000039\n",
      "epoch 262, loss: 0.000158\n",
      "epoch 263, loss: 0.000208\n",
      "epoch 264, loss: 0.000109\n",
      "epoch 265, loss: 0.000057\n",
      "epoch 266, loss: 0.000089\n",
      "epoch 267, loss: 0.000055\n",
      "epoch 268, loss: 0.000068\n",
      "epoch 269, loss: 0.000196\n",
      "epoch 270, loss: 0.000079\n",
      "epoch 271, loss: 0.000087\n",
      "epoch 272, loss: 0.000063\n",
      "epoch 273, loss: 0.000046\n",
      "epoch 274, loss: 0.000110\n",
      "epoch 275, loss: 0.000071\n",
      "epoch 276, loss: 0.000073\n",
      "epoch 277, loss: 0.000083\n",
      "epoch 278, loss: 0.000075\n",
      "epoch 279, loss: 0.000078\n",
      "epoch 280, loss: 0.000125\n",
      "epoch 281, loss: 0.000116\n",
      "epoch 282, loss: 0.000081\n",
      "epoch 283, loss: 0.000092\n",
      "epoch 284, loss: 0.000061\n",
      "epoch 285, loss: 0.000083\n",
      "epoch 286, loss: 0.000096\n",
      "epoch 287, loss: 0.000126\n",
      "epoch 288, loss: 0.000098\n",
      "epoch 289, loss: 0.000131\n",
      "epoch 290, loss: 0.000118\n",
      "epoch 291, loss: 0.000087\n",
      "epoch 292, loss: 0.000030\n",
      "epoch 293, loss: 0.000044\n",
      "epoch 294, loss: 0.000172\n",
      "epoch 295, loss: 0.000051\n",
      "epoch 296, loss: 0.000118\n",
      "epoch 297, loss: 0.000100\n",
      "epoch 298, loss: 0.000055\n",
      "epoch 299, loss: 0.000027\n",
      "epoch 300, loss: 0.000124\n",
      "epoch 301, loss: 0.000031\n",
      "epoch 302, loss: 0.000083\n",
      "epoch 303, loss: 0.000048\n",
      "epoch 304, loss: 0.000138\n",
      "epoch 305, loss: 0.000070\n",
      "epoch 306, loss: 0.000079\n",
      "epoch 307, loss: 0.000118\n",
      "epoch 308, loss: 0.000110\n",
      "epoch 309, loss: 0.000102\n",
      "epoch 310, loss: 0.000109\n",
      "epoch 311, loss: 0.000204\n",
      "epoch 312, loss: 0.000056\n",
      "epoch 313, loss: 0.000086\n",
      "epoch 314, loss: 0.000061\n",
      "epoch 315, loss: 0.000081\n",
      "epoch 316, loss: 0.000126\n",
      "epoch 317, loss: 0.000079\n",
      "epoch 318, loss: 0.000105\n",
      "epoch 319, loss: 0.000155\n",
      "epoch 320, loss: 0.000113\n",
      "epoch 321, loss: 0.000084\n",
      "epoch 322, loss: 0.000096\n",
      "epoch 323, loss: 0.000075\n",
      "epoch 324, loss: 0.000109\n",
      "epoch 325, loss: 0.000047\n",
      "epoch 326, loss: 0.000084\n",
      "epoch 327, loss: 0.000110\n",
      "epoch 328, loss: 0.000084\n",
      "epoch 329, loss: 0.000084\n",
      "epoch 330, loss: 0.000052\n",
      "epoch 331, loss: 0.000069\n",
      "epoch 332, loss: 0.000066\n",
      "epoch 333, loss: 0.000087\n",
      "epoch 334, loss: 0.000047\n",
      "epoch 335, loss: 0.000040\n",
      "epoch 336, loss: 0.000109\n",
      "epoch 337, loss: 0.000100\n",
      "epoch 338, loss: 0.000144\n",
      "epoch 339, loss: 0.000111\n",
      "epoch 340, loss: 0.000073\n",
      "epoch 341, loss: 0.000078\n",
      "epoch 342, loss: 0.000068\n",
      "epoch 343, loss: 0.000042\n",
      "epoch 344, loss: 0.000100\n",
      "epoch 345, loss: 0.000087\n",
      "epoch 346, loss: 0.000084\n",
      "epoch 347, loss: 0.000110\n",
      "epoch 348, loss: 0.000086\n",
      "epoch 349, loss: 0.000063\n",
      "epoch 350, loss: 0.000108\n",
      "epoch 351, loss: 0.000102\n",
      "epoch 352, loss: 0.000140\n",
      "epoch 353, loss: 0.000135\n",
      "epoch 354, loss: 0.000109\n",
      "epoch 355, loss: 0.000174\n",
      "epoch 356, loss: 0.000063\n",
      "epoch 357, loss: 0.000109\n",
      "epoch 358, loss: 0.000159\n",
      "epoch 359, loss: 0.000127\n",
      "epoch 360, loss: 0.000140\n",
      "epoch 361, loss: 0.000191\n",
      "epoch 362, loss: 0.000034\n",
      "epoch 363, loss: 0.000073\n",
      "epoch 364, loss: 0.000099\n",
      "epoch 365, loss: 0.000150\n",
      "epoch 366, loss: 0.000107\n",
      "epoch 367, loss: 0.000121\n",
      "epoch 368, loss: 0.000117\n",
      "epoch 369, loss: 0.000082\n",
      "epoch 370, loss: 0.000144\n",
      "epoch 371, loss: 0.000178\n",
      "epoch 372, loss: 0.000095\n",
      "epoch 373, loss: 0.000097\n",
      "epoch 374, loss: 0.000103\n",
      "epoch 375, loss: 0.000088\n",
      "epoch 376, loss: 0.000148\n",
      "epoch 377, loss: 0.000216\n",
      "epoch 378, loss: 0.000103\n",
      "epoch 379, loss: 0.000096\n",
      "epoch 380, loss: 0.000079\n",
      "epoch 381, loss: 0.000077\n",
      "epoch 382, loss: 0.000134\n",
      "epoch 383, loss: 0.000100\n",
      "epoch 384, loss: 0.000134\n",
      "epoch 385, loss: 0.000052\n",
      "epoch 386, loss: 0.000126\n",
      "epoch 387, loss: 0.000081\n",
      "epoch 388, loss: 0.000113\n",
      "epoch 389, loss: 0.000098\n",
      "epoch 390, loss: 0.000125\n",
      "epoch 391, loss: 0.000117\n",
      "epoch 392, loss: 0.000067\n",
      "epoch 393, loss: 0.000036\n",
      "epoch 394, loss: 0.000130\n",
      "epoch 395, loss: 0.000073\n",
      "epoch 396, loss: 0.000136\n",
      "epoch 397, loss: 0.000135\n",
      "epoch 398, loss: 0.000182\n",
      "epoch 399, loss: 0.000169\n",
      "epoch 400, loss: 0.000102\n",
      "epoch 401, loss: 0.000078\n",
      "epoch 402, loss: 0.000120\n",
      "epoch 403, loss: 0.000074\n",
      "epoch 404, loss: 0.000063\n",
      "epoch 405, loss: 0.000188\n",
      "epoch 406, loss: 0.000103\n",
      "epoch 407, loss: 0.000031\n",
      "epoch 408, loss: 0.000136\n",
      "epoch 409, loss: 0.000083\n",
      "epoch 410, loss: 0.000075\n",
      "epoch 411, loss: 0.000075\n",
      "epoch 412, loss: 0.000108\n",
      "epoch 413, loss: 0.000070\n",
      "epoch 414, loss: 0.000042\n",
      "epoch 415, loss: 0.000068\n",
      "epoch 416, loss: 0.000124\n",
      "epoch 417, loss: 0.000053\n",
      "epoch 418, loss: 0.000103\n",
      "epoch 419, loss: 0.000215\n",
      "epoch 420, loss: 0.000156\n",
      "epoch 421, loss: 0.000045\n",
      "epoch 422, loss: 0.000088\n",
      "epoch 423, loss: 0.000076\n",
      "epoch 424, loss: 0.000056\n",
      "epoch 425, loss: 0.000021\n",
      "epoch 426, loss: 0.000096\n",
      "epoch 427, loss: 0.000175\n",
      "epoch 428, loss: 0.000094\n",
      "epoch 429, loss: 0.000149\n",
      "epoch 430, loss: 0.000173\n",
      "epoch 431, loss: 0.000054\n",
      "epoch 432, loss: 0.000109\n",
      "epoch 433, loss: 0.000100\n",
      "epoch 434, loss: 0.000061\n",
      "epoch 435, loss: 0.000241\n",
      "epoch 436, loss: 0.000101\n",
      "epoch 437, loss: 0.000066\n",
      "epoch 438, loss: 0.000032\n",
      "epoch 439, loss: 0.000056\n",
      "epoch 440, loss: 0.000115\n",
      "epoch 441, loss: 0.000115\n",
      "epoch 442, loss: 0.000090\n",
      "epoch 443, loss: 0.000091\n",
      "epoch 444, loss: 0.000100\n",
      "epoch 445, loss: 0.000097\n",
      "epoch 446, loss: 0.000063\n",
      "epoch 447, loss: 0.000053\n",
      "epoch 448, loss: 0.000134\n",
      "epoch 449, loss: 0.000087\n",
      "epoch 450, loss: 0.000114\n",
      "epoch 451, loss: 0.000213\n",
      "epoch 452, loss: 0.000170\n",
      "epoch 453, loss: 0.000092\n",
      "epoch 454, loss: 0.000107\n",
      "epoch 455, loss: 0.000117\n",
      "epoch 456, loss: 0.000119\n",
      "epoch 457, loss: 0.000175\n",
      "epoch 458, loss: 0.000064\n",
      "epoch 459, loss: 0.000038\n",
      "epoch 460, loss: 0.000034\n",
      "epoch 461, loss: 0.000201\n",
      "epoch 462, loss: 0.000131\n",
      "epoch 463, loss: 0.000050\n",
      "epoch 464, loss: 0.000025\n",
      "epoch 465, loss: 0.000186\n",
      "epoch 466, loss: 0.000134\n",
      "epoch 467, loss: 0.000130\n",
      "epoch 468, loss: 0.000080\n",
      "epoch 469, loss: 0.000171\n",
      "epoch 470, loss: 0.000174\n",
      "epoch 471, loss: 0.000096\n",
      "epoch 472, loss: 0.000072\n",
      "epoch 473, loss: 0.000038\n",
      "epoch 474, loss: 0.000070\n",
      "epoch 475, loss: 0.000069\n",
      "epoch 476, loss: 0.000087\n",
      "epoch 477, loss: 0.000137\n",
      "epoch 478, loss: 0.000126\n",
      "epoch 479, loss: 0.000068\n",
      "epoch 480, loss: 0.000060\n",
      "epoch 481, loss: 0.000154\n",
      "epoch 482, loss: 0.000055\n",
      "epoch 483, loss: 0.000134\n",
      "epoch 484, loss: 0.000112\n",
      "epoch 485, loss: 0.000102\n",
      "epoch 486, loss: 0.000164\n",
      "epoch 487, loss: 0.000034\n",
      "epoch 488, loss: 0.000123\n",
      "epoch 489, loss: 0.000069\n",
      "epoch 490, loss: 0.000039\n",
      "epoch 491, loss: 0.000066\n",
      "epoch 492, loss: 0.000053\n",
      "epoch 493, loss: 0.000216\n",
      "epoch 494, loss: 0.000111\n",
      "epoch 495, loss: 0.000121\n",
      "epoch 496, loss: 0.000104\n",
      "epoch 497, loss: 0.000082\n",
      "epoch 498, loss: 0.000084\n",
      "epoch 499, loss: 0.000085\n",
      "epoch 500, loss: 0.000042\n",
      "epoch 501, loss: 0.000066\n",
      "epoch 502, loss: 0.000130\n",
      "epoch 503, loss: 0.000083\n",
      "epoch 504, loss: 0.000110\n",
      "epoch 505, loss: 0.000071\n",
      "epoch 506, loss: 0.000165\n",
      "epoch 507, loss: 0.000063\n",
      "epoch 508, loss: 0.000179\n",
      "epoch 509, loss: 0.000087\n",
      "epoch 510, loss: 0.000067\n",
      "epoch 511, loss: 0.000179\n",
      "epoch 512, loss: 0.000147\n",
      "epoch 513, loss: 0.000070\n",
      "epoch 514, loss: 0.000097\n",
      "epoch 515, loss: 0.000059\n",
      "epoch 516, loss: 0.000057\n",
      "epoch 517, loss: 0.000075\n",
      "epoch 518, loss: 0.000113\n",
      "epoch 519, loss: 0.000054\n",
      "epoch 520, loss: 0.000088\n",
      "epoch 521, loss: 0.000100\n",
      "epoch 522, loss: 0.000049\n",
      "epoch 523, loss: 0.000069\n",
      "epoch 524, loss: 0.000119\n",
      "epoch 525, loss: 0.000112\n",
      "epoch 526, loss: 0.000065\n",
      "epoch 527, loss: 0.000106\n",
      "epoch 528, loss: 0.000200\n",
      "epoch 529, loss: 0.000092\n",
      "epoch 530, loss: 0.000130\n",
      "epoch 531, loss: 0.000127\n",
      "epoch 532, loss: 0.000114\n",
      "epoch 533, loss: 0.000055\n",
      "epoch 534, loss: 0.000087\n",
      "epoch 535, loss: 0.000040\n",
      "epoch 536, loss: 0.000128\n",
      "epoch 537, loss: 0.000070\n",
      "epoch 538, loss: 0.000132\n",
      "epoch 539, loss: 0.000084\n",
      "epoch 540, loss: 0.000047\n",
      "epoch 541, loss: 0.000106\n",
      "epoch 542, loss: 0.000154\n",
      "epoch 543, loss: 0.000132\n",
      "epoch 544, loss: 0.000066\n",
      "epoch 545, loss: 0.000087\n",
      "epoch 546, loss: 0.000087\n",
      "epoch 547, loss: 0.000050\n",
      "epoch 548, loss: 0.000106\n",
      "epoch 549, loss: 0.000054\n",
      "epoch 550, loss: 0.000101\n",
      "epoch 551, loss: 0.000121\n",
      "epoch 552, loss: 0.000132\n",
      "epoch 553, loss: 0.000137\n",
      "epoch 554, loss: 0.000098\n",
      "epoch 555, loss: 0.000087\n",
      "epoch 556, loss: 0.000176\n",
      "epoch 557, loss: 0.000061\n",
      "epoch 558, loss: 0.000072\n",
      "epoch 559, loss: 0.000184\n",
      "epoch 560, loss: 0.000072\n",
      "epoch 561, loss: 0.000104\n",
      "epoch 562, loss: 0.000171\n",
      "epoch 563, loss: 0.000139\n",
      "epoch 564, loss: 0.000124\n",
      "epoch 565, loss: 0.000034\n",
      "epoch 566, loss: 0.000105\n",
      "epoch 567, loss: 0.000083\n",
      "epoch 568, loss: 0.000084\n",
      "epoch 569, loss: 0.000104\n",
      "epoch 570, loss: 0.000082\n",
      "epoch 571, loss: 0.000109\n",
      "epoch 572, loss: 0.000065\n",
      "epoch 573, loss: 0.000094\n",
      "epoch 574, loss: 0.000077\n",
      "epoch 575, loss: 0.000110\n",
      "epoch 576, loss: 0.000109\n",
      "epoch 577, loss: 0.000069\n",
      "epoch 578, loss: 0.000093\n",
      "epoch 579, loss: 0.000154\n",
      "epoch 580, loss: 0.000202\n",
      "epoch 581, loss: 0.000127\n",
      "epoch 582, loss: 0.000114\n",
      "epoch 583, loss: 0.000096\n",
      "epoch 584, loss: 0.000086\n",
      "epoch 585, loss: 0.000154\n",
      "epoch 586, loss: 0.000129\n",
      "epoch 587, loss: 0.000043\n",
      "epoch 588, loss: 0.000100\n",
      "epoch 589, loss: 0.000166\n",
      "epoch 590, loss: 0.000096\n",
      "epoch 591, loss: 0.000088\n",
      "epoch 592, loss: 0.000114\n",
      "epoch 593, loss: 0.000110\n",
      "epoch 594, loss: 0.000076\n",
      "epoch 595, loss: 0.000149\n",
      "epoch 596, loss: 0.000117\n",
      "epoch 597, loss: 0.000061\n",
      "epoch 598, loss: 0.000091\n",
      "epoch 599, loss: 0.000036\n",
      "epoch 600, loss: 0.000060\n",
      "epoch 601, loss: 0.000095\n",
      "epoch 602, loss: 0.000080\n",
      "epoch 603, loss: 0.000072\n",
      "epoch 604, loss: 0.000076\n",
      "epoch 605, loss: 0.000138\n",
      "epoch 606, loss: 0.000186\n",
      "epoch 607, loss: 0.000084\n",
      "epoch 608, loss: 0.000056\n",
      "epoch 609, loss: 0.000061\n",
      "epoch 610, loss: 0.000074\n",
      "epoch 611, loss: 0.000129\n",
      "epoch 612, loss: 0.000088\n",
      "epoch 613, loss: 0.000061\n",
      "epoch 614, loss: 0.000042\n",
      "epoch 615, loss: 0.000091\n",
      "epoch 616, loss: 0.000080\n",
      "epoch 617, loss: 0.000147\n",
      "epoch 618, loss: 0.000074\n",
      "epoch 619, loss: 0.000074\n",
      "epoch 620, loss: 0.000069\n",
      "epoch 621, loss: 0.000118\n",
      "epoch 622, loss: 0.000094\n",
      "epoch 623, loss: 0.000082\n",
      "epoch 624, loss: 0.000144\n",
      "epoch 625, loss: 0.000104\n",
      "epoch 626, loss: 0.000063\n",
      "epoch 627, loss: 0.000092\n",
      "epoch 628, loss: 0.000105\n",
      "epoch 629, loss: 0.000064\n",
      "epoch 630, loss: 0.000074\n",
      "epoch 631, loss: 0.000114\n",
      "epoch 632, loss: 0.000098\n",
      "epoch 633, loss: 0.000049\n",
      "epoch 634, loss: 0.000103\n",
      "epoch 635, loss: 0.000073\n",
      "epoch 636, loss: 0.000164\n",
      "epoch 637, loss: 0.000111\n",
      "epoch 638, loss: 0.000054\n",
      "epoch 639, loss: 0.000075\n",
      "epoch 640, loss: 0.000163\n",
      "epoch 641, loss: 0.000117\n",
      "epoch 642, loss: 0.000075\n",
      "epoch 643, loss: 0.000072\n",
      "epoch 644, loss: 0.000082\n",
      "epoch 645, loss: 0.000219\n",
      "epoch 646, loss: 0.000143\n",
      "epoch 647, loss: 0.000070\n",
      "epoch 648, loss: 0.000051\n",
      "epoch 649, loss: 0.000253\n",
      "epoch 650, loss: 0.000054\n",
      "epoch 651, loss: 0.000159\n",
      "epoch 652, loss: 0.000130\n",
      "epoch 653, loss: 0.000070\n",
      "epoch 654, loss: 0.000129\n",
      "epoch 655, loss: 0.000080\n",
      "epoch 656, loss: 0.000051\n",
      "epoch 657, loss: 0.000112\n",
      "epoch 658, loss: 0.000079\n",
      "epoch 659, loss: 0.000091\n",
      "epoch 660, loss: 0.000048\n",
      "epoch 661, loss: 0.000036\n",
      "epoch 662, loss: 0.000086\n",
      "epoch 663, loss: 0.000140\n",
      "epoch 664, loss: 0.000104\n",
      "epoch 665, loss: 0.000153\n",
      "epoch 666, loss: 0.000080\n",
      "epoch 667, loss: 0.000124\n",
      "epoch 668, loss: 0.000162\n",
      "epoch 669, loss: 0.000174\n",
      "epoch 670, loss: 0.000079\n",
      "epoch 671, loss: 0.000030\n",
      "epoch 672, loss: 0.000067\n",
      "epoch 673, loss: 0.000049\n",
      "epoch 674, loss: 0.000098\n",
      "epoch 675, loss: 0.000220\n",
      "epoch 676, loss: 0.000059\n",
      "epoch 677, loss: 0.000117\n",
      "epoch 678, loss: 0.000098\n",
      "epoch 679, loss: 0.000100\n",
      "epoch 680, loss: 0.000137\n",
      "epoch 681, loss: 0.000063\n",
      "epoch 682, loss: 0.000072\n",
      "epoch 683, loss: 0.000111\n",
      "epoch 684, loss: 0.000065\n",
      "epoch 685, loss: 0.000062\n",
      "epoch 686, loss: 0.000033\n",
      "epoch 687, loss: 0.000044\n",
      "epoch 688, loss: 0.000073\n",
      "epoch 689, loss: 0.000066\n",
      "epoch 690, loss: 0.000078\n",
      "epoch 691, loss: 0.000185\n",
      "epoch 692, loss: 0.000119\n",
      "epoch 693, loss: 0.000059\n",
      "epoch 694, loss: 0.000047\n",
      "epoch 695, loss: 0.000091\n",
      "epoch 696, loss: 0.000262\n",
      "epoch 697, loss: 0.000113\n",
      "epoch 698, loss: 0.000047\n",
      "epoch 699, loss: 0.000177\n",
      "epoch 700, loss: 0.000051\n",
      "epoch 701, loss: 0.000052\n",
      "epoch 702, loss: 0.000166\n",
      "epoch 703, loss: 0.000098\n",
      "epoch 704, loss: 0.000033\n",
      "epoch 705, loss: 0.000029\n",
      "epoch 706, loss: 0.000034\n",
      "epoch 707, loss: 0.000075\n",
      "epoch 708, loss: 0.000189\n",
      "epoch 709, loss: 0.000125\n",
      "epoch 710, loss: 0.000160\n",
      "epoch 711, loss: 0.000057\n",
      "epoch 712, loss: 0.000071\n",
      "epoch 713, loss: 0.000043\n",
      "epoch 714, loss: 0.000025\n",
      "epoch 715, loss: 0.000025\n",
      "epoch 716, loss: 0.000093\n",
      "epoch 717, loss: 0.000089\n",
      "epoch 718, loss: 0.000060\n",
      "epoch 719, loss: 0.000090\n",
      "epoch 720, loss: 0.000111\n",
      "epoch 721, loss: 0.000127\n",
      "epoch 722, loss: 0.000157\n",
      "epoch 723, loss: 0.000087\n",
      "epoch 724, loss: 0.000160\n",
      "epoch 725, loss: 0.000141\n",
      "epoch 726, loss: 0.000116\n",
      "epoch 727, loss: 0.000074\n",
      "epoch 728, loss: 0.000141\n",
      "epoch 729, loss: 0.000065\n",
      "epoch 730, loss: 0.000227\n",
      "epoch 731, loss: 0.000121\n",
      "epoch 732, loss: 0.000146\n",
      "epoch 733, loss: 0.000109\n",
      "epoch 734, loss: 0.000095\n",
      "epoch 735, loss: 0.000072\n",
      "epoch 736, loss: 0.000050\n",
      "epoch 737, loss: 0.000134\n",
      "epoch 738, loss: 0.000068\n",
      "epoch 739, loss: 0.000127\n",
      "epoch 740, loss: 0.000077\n",
      "epoch 741, loss: 0.000063\n",
      "epoch 742, loss: 0.000040\n",
      "epoch 743, loss: 0.000209\n",
      "epoch 744, loss: 0.000036\n",
      "epoch 745, loss: 0.000206\n",
      "epoch 746, loss: 0.000091\n",
      "epoch 747, loss: 0.000019\n",
      "epoch 748, loss: 0.000047\n",
      "epoch 749, loss: 0.000102\n",
      "epoch 750, loss: 0.000087\n",
      "epoch 751, loss: 0.000088\n",
      "epoch 752, loss: 0.000096\n",
      "epoch 753, loss: 0.000097\n",
      "epoch 754, loss: 0.000119\n",
      "epoch 755, loss: 0.000053\n",
      "epoch 756, loss: 0.000071\n",
      "epoch 757, loss: 0.000090\n",
      "epoch 758, loss: 0.000098\n",
      "epoch 759, loss: 0.000072\n",
      "epoch 760, loss: 0.000151\n",
      "epoch 761, loss: 0.000071\n",
      "epoch 762, loss: 0.000069\n",
      "epoch 763, loss: 0.000124\n",
      "epoch 764, loss: 0.000103\n",
      "epoch 765, loss: 0.000059\n",
      "epoch 766, loss: 0.000088\n",
      "epoch 767, loss: 0.000078\n",
      "epoch 768, loss: 0.000049\n",
      "epoch 769, loss: 0.000082\n",
      "epoch 770, loss: 0.000090\n",
      "epoch 771, loss: 0.000133\n",
      "epoch 772, loss: 0.000028\n",
      "epoch 773, loss: 0.000136\n",
      "epoch 774, loss: 0.000051\n",
      "epoch 775, loss: 0.000105\n",
      "epoch 776, loss: 0.000122\n",
      "epoch 777, loss: 0.000116\n",
      "epoch 778, loss: 0.000070\n",
      "epoch 779, loss: 0.000024\n",
      "epoch 780, loss: 0.000125\n",
      "epoch 781, loss: 0.000071\n",
      "epoch 782, loss: 0.000094\n",
      "epoch 783, loss: 0.000122\n",
      "epoch 784, loss: 0.000073\n",
      "epoch 785, loss: 0.000148\n",
      "epoch 786, loss: 0.000051\n",
      "epoch 787, loss: 0.000148\n",
      "epoch 788, loss: 0.000139\n",
      "epoch 789, loss: 0.000206\n",
      "epoch 790, loss: 0.000161\n",
      "epoch 791, loss: 0.000041\n",
      "epoch 792, loss: 0.000073\n",
      "epoch 793, loss: 0.000111\n",
      "epoch 794, loss: 0.000090\n",
      "epoch 795, loss: 0.000044\n",
      "epoch 796, loss: 0.000063\n",
      "epoch 797, loss: 0.000092\n",
      "epoch 798, loss: 0.000066\n",
      "epoch 799, loss: 0.000037\n",
      "epoch 800, loss: 0.000097\n",
      "epoch 801, loss: 0.000132\n",
      "epoch 802, loss: 0.000053\n",
      "epoch 803, loss: 0.000056\n",
      "epoch 804, loss: 0.000066\n",
      "epoch 805, loss: 0.000100\n",
      "epoch 806, loss: 0.000074\n",
      "epoch 807, loss: 0.000048\n",
      "epoch 808, loss: 0.000092\n",
      "epoch 809, loss: 0.000069\n",
      "epoch 810, loss: 0.000077\n",
      "epoch 811, loss: 0.000135\n",
      "epoch 812, loss: 0.000085\n",
      "epoch 813, loss: 0.000072\n",
      "epoch 814, loss: 0.000132\n",
      "epoch 815, loss: 0.000024\n",
      "epoch 816, loss: 0.000061\n",
      "epoch 817, loss: 0.000110\n",
      "epoch 818, loss: 0.000200\n",
      "epoch 819, loss: 0.000127\n",
      "epoch 820, loss: 0.000087\n",
      "epoch 821, loss: 0.000165\n",
      "epoch 822, loss: 0.000101\n",
      "epoch 823, loss: 0.000080\n",
      "epoch 824, loss: 0.000062\n",
      "epoch 825, loss: 0.000066\n",
      "epoch 826, loss: 0.000125\n",
      "epoch 827, loss: 0.000124\n",
      "epoch 828, loss: 0.000140\n",
      "epoch 829, loss: 0.000064\n",
      "epoch 830, loss: 0.000080\n",
      "epoch 831, loss: 0.000088\n",
      "epoch 832, loss: 0.000079\n",
      "epoch 833, loss: 0.000119\n",
      "epoch 834, loss: 0.000158\n",
      "epoch 835, loss: 0.000197\n",
      "epoch 836, loss: 0.000089\n",
      "epoch 837, loss: 0.000039\n",
      "epoch 838, loss: 0.000200\n",
      "epoch 839, loss: 0.000177\n",
      "epoch 840, loss: 0.000061\n",
      "epoch 841, loss: 0.000094\n",
      "epoch 842, loss: 0.000139\n",
      "epoch 843, loss: 0.000136\n",
      "epoch 844, loss: 0.000109\n",
      "epoch 845, loss: 0.000077\n",
      "epoch 846, loss: 0.000085\n",
      "epoch 847, loss: 0.000087\n",
      "epoch 848, loss: 0.000203\n",
      "epoch 849, loss: 0.000029\n",
      "epoch 850, loss: 0.000099\n",
      "epoch 851, loss: 0.000205\n",
      "epoch 852, loss: 0.000168\n",
      "epoch 853, loss: 0.000067\n",
      "epoch 854, loss: 0.000140\n",
      "epoch 855, loss: 0.000059\n",
      "epoch 856, loss: 0.000078\n",
      "epoch 857, loss: 0.000187\n",
      "epoch 858, loss: 0.000197\n",
      "epoch 859, loss: 0.000113\n",
      "epoch 860, loss: 0.000032\n",
      "epoch 861, loss: 0.000133\n",
      "epoch 862, loss: 0.000039\n",
      "epoch 863, loss: 0.000067\n",
      "epoch 864, loss: 0.000090\n",
      "epoch 865, loss: 0.000068\n",
      "epoch 866, loss: 0.000084\n",
      "epoch 867, loss: 0.000122\n",
      "epoch 868, loss: 0.000069\n",
      "epoch 869, loss: 0.000057\n",
      "epoch 870, loss: 0.000071\n",
      "epoch 871, loss: 0.000193\n",
      "epoch 872, loss: 0.000060\n",
      "epoch 873, loss: 0.000133\n",
      "epoch 874, loss: 0.000093\n",
      "epoch 875, loss: 0.000048\n",
      "epoch 876, loss: 0.000221\n",
      "epoch 877, loss: 0.000102\n",
      "epoch 878, loss: 0.000174\n",
      "epoch 879, loss: 0.000118\n",
      "epoch 880, loss: 0.000186\n",
      "epoch 881, loss: 0.000057\n",
      "epoch 882, loss: 0.000098\n",
      "epoch 883, loss: 0.000067\n",
      "epoch 884, loss: 0.000041\n",
      "epoch 885, loss: 0.000053\n",
      "epoch 886, loss: 0.000056\n",
      "epoch 887, loss: 0.000096\n",
      "epoch 888, loss: 0.000085\n",
      "epoch 889, loss: 0.000077\n",
      "epoch 890, loss: 0.000122\n",
      "epoch 891, loss: 0.000120\n",
      "epoch 892, loss: 0.000105\n",
      "epoch 893, loss: 0.000053\n",
      "epoch 894, loss: 0.000065\n",
      "epoch 895, loss: 0.000103\n",
      "epoch 896, loss: 0.000098\n",
      "epoch 897, loss: 0.000093\n",
      "epoch 898, loss: 0.000109\n",
      "epoch 899, loss: 0.000086\n",
      "epoch 900, loss: 0.000078\n",
      "epoch 901, loss: 0.000164\n",
      "epoch 902, loss: 0.000164\n",
      "epoch 903, loss: 0.000151\n",
      "epoch 904, loss: 0.000093\n",
      "epoch 905, loss: 0.000137\n",
      "epoch 906, loss: 0.000079\n",
      "epoch 907, loss: 0.000067\n",
      "epoch 908, loss: 0.000094\n",
      "epoch 909, loss: 0.000114\n",
      "epoch 910, loss: 0.000178\n",
      "epoch 911, loss: 0.000111\n",
      "epoch 912, loss: 0.000098\n",
      "epoch 913, loss: 0.000043\n",
      "epoch 914, loss: 0.000104\n",
      "epoch 915, loss: 0.000039\n",
      "epoch 916, loss: 0.000064\n",
      "epoch 917, loss: 0.000150\n",
      "epoch 918, loss: 0.000101\n",
      "epoch 919, loss: 0.000049\n",
      "epoch 920, loss: 0.000034\n",
      "epoch 921, loss: 0.000137\n",
      "epoch 922, loss: 0.000106\n",
      "epoch 923, loss: 0.000070\n",
      "epoch 924, loss: 0.000039\n",
      "epoch 925, loss: 0.000231\n",
      "epoch 926, loss: 0.000124\n",
      "epoch 927, loss: 0.000157\n",
      "epoch 928, loss: 0.000092\n",
      "epoch 929, loss: 0.000120\n",
      "epoch 930, loss: 0.000110\n",
      "epoch 931, loss: 0.000103\n",
      "epoch 932, loss: 0.000123\n",
      "epoch 933, loss: 0.000124\n",
      "epoch 934, loss: 0.000101\n",
      "epoch 935, loss: 0.000090\n",
      "epoch 936, loss: 0.000093\n",
      "epoch 937, loss: 0.000103\n",
      "epoch 938, loss: 0.000058\n",
      "epoch 939, loss: 0.000051\n",
      "epoch 940, loss: 0.000150\n",
      "epoch 941, loss: 0.000154\n",
      "epoch 942, loss: 0.000042\n",
      "epoch 943, loss: 0.000085\n",
      "epoch 944, loss: 0.000082\n",
      "epoch 945, loss: 0.000097\n",
      "epoch 946, loss: 0.000119\n",
      "epoch 947, loss: 0.000095\n",
      "epoch 948, loss: 0.000051\n",
      "epoch 949, loss: 0.000127\n",
      "epoch 950, loss: 0.000241\n",
      "epoch 951, loss: 0.000071\n",
      "epoch 952, loss: 0.000071\n",
      "epoch 953, loss: 0.000076\n",
      "epoch 954, loss: 0.000112\n",
      "epoch 955, loss: 0.000091\n",
      "epoch 956, loss: 0.000045\n",
      "epoch 957, loss: 0.000102\n",
      "epoch 958, loss: 0.000057\n",
      "epoch 959, loss: 0.000071\n",
      "epoch 960, loss: 0.000033\n",
      "epoch 961, loss: 0.000054\n",
      "epoch 962, loss: 0.000071\n",
      "epoch 963, loss: 0.000126\n",
      "epoch 964, loss: 0.000111\n",
      "epoch 965, loss: 0.000068\n",
      "epoch 966, loss: 0.000114\n",
      "epoch 967, loss: 0.000057\n",
      "epoch 968, loss: 0.000047\n",
      "epoch 969, loss: 0.000154\n",
      "epoch 970, loss: 0.000056\n",
      "epoch 971, loss: 0.000087\n",
      "epoch 972, loss: 0.000099\n",
      "epoch 973, loss: 0.000102\n",
      "epoch 974, loss: 0.000073\n",
      "epoch 975, loss: 0.000083\n",
      "epoch 976, loss: 0.000155\n",
      "epoch 977, loss: 0.000065\n",
      "epoch 978, loss: 0.000069\n",
      "epoch 979, loss: 0.000142\n",
      "epoch 980, loss: 0.000084\n",
      "epoch 981, loss: 0.000098\n",
      "epoch 982, loss: 0.000056\n",
      "epoch 983, loss: 0.000104\n",
      "epoch 984, loss: 0.000100\n",
      "epoch 985, loss: 0.000065\n",
      "epoch 986, loss: 0.000135\n",
      "epoch 987, loss: 0.000089\n",
      "epoch 988, loss: 0.000053\n",
      "epoch 989, loss: 0.000045\n",
      "epoch 990, loss: 0.000072\n",
      "epoch 991, loss: 0.000243\n",
      "epoch 992, loss: 0.000209\n",
      "epoch 993, loss: 0.000109\n",
      "epoch 994, loss: 0.000059\n",
      "epoch 995, loss: 0.000040\n",
      "epoch 996, loss: 0.000106\n",
      "epoch 997, loss: 0.000122\n",
      "epoch 998, loss: 0.000058\n",
      "epoch 999, loss: 0.000094\n",
      "epoch 1000, loss: 0.000084\n",
      "epoch 1001, loss: 0.000186\n",
      "epoch 1002, loss: 0.000079\n",
      "epoch 1003, loss: 0.000082\n",
      "epoch 1004, loss: 0.000153\n",
      "epoch 1005, loss: 0.000090\n",
      "epoch 1006, loss: 0.000068\n",
      "epoch 1007, loss: 0.000098\n",
      "epoch 1008, loss: 0.000114\n",
      "epoch 1009, loss: 0.000188\n",
      "epoch 1010, loss: 0.000052\n",
      "epoch 1011, loss: 0.000048\n",
      "epoch 1012, loss: 0.000115\n",
      "epoch 1013, loss: 0.000204\n",
      "epoch 1014, loss: 0.000123\n",
      "epoch 1015, loss: 0.000099\n",
      "epoch 1016, loss: 0.000087\n",
      "epoch 1017, loss: 0.000132\n",
      "epoch 1018, loss: 0.000113\n",
      "epoch 1019, loss: 0.000093\n",
      "epoch 1020, loss: 0.000137\n",
      "epoch 1021, loss: 0.000047\n",
      "epoch 1022, loss: 0.000054\n",
      "epoch 1023, loss: 0.000068\n",
      "epoch 1024, loss: 0.000064\n",
      "epoch 1025, loss: 0.000115\n",
      "epoch 1026, loss: 0.000079\n",
      "epoch 1027, loss: 0.000139\n",
      "epoch 1028, loss: 0.000118\n",
      "epoch 1029, loss: 0.000070\n",
      "epoch 1030, loss: 0.000050\n",
      "epoch 1031, loss: 0.000090\n",
      "epoch 1032, loss: 0.000075\n",
      "epoch 1033, loss: 0.000081\n",
      "epoch 1034, loss: 0.000110\n",
      "epoch 1035, loss: 0.000140\n",
      "epoch 1036, loss: 0.000115\n",
      "epoch 1037, loss: 0.000187\n",
      "epoch 1038, loss: 0.000082\n",
      "epoch 1039, loss: 0.000075\n",
      "epoch 1040, loss: 0.000050\n",
      "epoch 1041, loss: 0.000057\n",
      "epoch 1042, loss: 0.000093\n",
      "epoch 1043, loss: 0.000053\n",
      "epoch 1044, loss: 0.000056\n",
      "epoch 1045, loss: 0.000071\n",
      "epoch 1046, loss: 0.000195\n",
      "epoch 1047, loss: 0.000223\n",
      "epoch 1048, loss: 0.000054\n",
      "epoch 1049, loss: 0.000046\n",
      "epoch 1050, loss: 0.000052\n",
      "epoch 1051, loss: 0.000130\n",
      "epoch 1052, loss: 0.000149\n",
      "epoch 1053, loss: 0.000120\n",
      "epoch 1054, loss: 0.000089\n",
      "epoch 1055, loss: 0.000105\n",
      "epoch 1056, loss: 0.000172\n",
      "epoch 1057, loss: 0.000092\n",
      "epoch 1058, loss: 0.000178\n",
      "epoch 1059, loss: 0.000101\n",
      "epoch 1060, loss: 0.000063\n",
      "epoch 1061, loss: 0.000026\n",
      "epoch 1062, loss: 0.000049\n",
      "epoch 1063, loss: 0.000061\n",
      "epoch 1064, loss: 0.000106\n",
      "epoch 1065, loss: 0.000111\n",
      "epoch 1066, loss: 0.000123\n",
      "epoch 1067, loss: 0.000118\n",
      "epoch 1068, loss: 0.000104\n",
      "epoch 1069, loss: 0.000019\n",
      "epoch 1070, loss: 0.000105\n",
      "epoch 1071, loss: 0.000098\n",
      "epoch 1072, loss: 0.000182\n",
      "epoch 1073, loss: 0.000160\n",
      "epoch 1074, loss: 0.000070\n",
      "epoch 1075, loss: 0.000107\n",
      "epoch 1076, loss: 0.000126\n",
      "epoch 1077, loss: 0.000067\n",
      "epoch 1078, loss: 0.000066\n",
      "epoch 1079, loss: 0.000068\n",
      "epoch 1080, loss: 0.000090\n",
      "epoch 1081, loss: 0.000136\n",
      "epoch 1082, loss: 0.000039\n",
      "epoch 1083, loss: 0.000093\n",
      "epoch 1084, loss: 0.000129\n",
      "epoch 1085, loss: 0.000089\n",
      "epoch 1086, loss: 0.000100\n",
      "epoch 1087, loss: 0.000114\n",
      "epoch 1088, loss: 0.000140\n",
      "epoch 1089, loss: 0.000091\n",
      "epoch 1090, loss: 0.000054\n",
      "epoch 1091, loss: 0.000096\n",
      "epoch 1092, loss: 0.000045\n",
      "epoch 1093, loss: 0.000151\n",
      "epoch 1094, loss: 0.000112\n",
      "epoch 1095, loss: 0.000071\n",
      "epoch 1096, loss: 0.000116\n",
      "epoch 1097, loss: 0.000075\n",
      "epoch 1098, loss: 0.000075\n",
      "epoch 1099, loss: 0.000074\n",
      "epoch 1100, loss: 0.000107\n",
      "epoch 1101, loss: 0.000109\n",
      "epoch 1102, loss: 0.000112\n",
      "epoch 1103, loss: 0.000119\n",
      "epoch 1104, loss: 0.000080\n",
      "epoch 1105, loss: 0.000130\n",
      "epoch 1106, loss: 0.000129\n",
      "epoch 1107, loss: 0.000042\n",
      "epoch 1108, loss: 0.000052\n",
      "epoch 1109, loss: 0.000148\n",
      "epoch 1110, loss: 0.000189\n",
      "epoch 1111, loss: 0.000069\n",
      "epoch 1112, loss: 0.000081\n",
      "epoch 1113, loss: 0.000050\n",
      "epoch 1114, loss: 0.000114\n",
      "epoch 1115, loss: 0.000051\n",
      "epoch 1116, loss: 0.000069\n",
      "epoch 1117, loss: 0.000119\n",
      "epoch 1118, loss: 0.000096\n",
      "epoch 1119, loss: 0.000235\n",
      "epoch 1120, loss: 0.000166\n",
      "epoch 1121, loss: 0.000074\n",
      "epoch 1122, loss: 0.000166\n",
      "epoch 1123, loss: 0.000182\n",
      "epoch 1124, loss: 0.000118\n",
      "epoch 1125, loss: 0.000106\n",
      "epoch 1126, loss: 0.000117\n",
      "epoch 1127, loss: 0.000230\n",
      "epoch 1128, loss: 0.000093\n",
      "epoch 1129, loss: 0.000166\n",
      "epoch 1130, loss: 0.000154\n",
      "epoch 1131, loss: 0.000157\n",
      "epoch 1132, loss: 0.000087\n",
      "epoch 1133, loss: 0.000112\n",
      "epoch 1134, loss: 0.000151\n",
      "epoch 1135, loss: 0.000048\n",
      "epoch 1136, loss: 0.000069\n",
      "epoch 1137, loss: 0.000111\n",
      "epoch 1138, loss: 0.000126\n",
      "epoch 1139, loss: 0.000012\n",
      "epoch 1140, loss: 0.000126\n",
      "epoch 1141, loss: 0.000108\n",
      "epoch 1142, loss: 0.000138\n",
      "epoch 1143, loss: 0.000091\n",
      "epoch 1144, loss: 0.000129\n",
      "epoch 1145, loss: 0.000112\n",
      "epoch 1146, loss: 0.000072\n",
      "epoch 1147, loss: 0.000083\n",
      "epoch 1148, loss: 0.000103\n",
      "epoch 1149, loss: 0.000172\n",
      "epoch 1150, loss: 0.000115\n",
      "epoch 1151, loss: 0.000204\n",
      "epoch 1152, loss: 0.000065\n",
      "epoch 1153, loss: 0.000067\n",
      "epoch 1154, loss: 0.000067\n",
      "epoch 1155, loss: 0.000073\n",
      "epoch 1156, loss: 0.000258\n",
      "epoch 1157, loss: 0.000083\n",
      "epoch 1158, loss: 0.000084\n",
      "epoch 1159, loss: 0.000125\n",
      "epoch 1160, loss: 0.000068\n",
      "epoch 1161, loss: 0.000110\n",
      "epoch 1162, loss: 0.000118\n",
      "epoch 1163, loss: 0.000115\n",
      "epoch 1164, loss: 0.000245\n",
      "epoch 1165, loss: 0.000075\n",
      "epoch 1166, loss: 0.000075\n",
      "epoch 1167, loss: 0.000134\n",
      "epoch 1168, loss: 0.000107\n",
      "epoch 1169, loss: 0.000030\n",
      "epoch 1170, loss: 0.000103\n",
      "epoch 1171, loss: 0.000085\n",
      "epoch 1172, loss: 0.000138\n",
      "epoch 1173, loss: 0.000110\n",
      "epoch 1174, loss: 0.000075\n",
      "epoch 1175, loss: 0.000121\n",
      "epoch 1176, loss: 0.000083\n",
      "epoch 1177, loss: 0.000164\n",
      "epoch 1178, loss: 0.000117\n",
      "epoch 1179, loss: 0.000047\n",
      "epoch 1180, loss: 0.000131\n",
      "epoch 1181, loss: 0.000053\n",
      "epoch 1182, loss: 0.000117\n",
      "epoch 1183, loss: 0.000115\n",
      "epoch 1184, loss: 0.000125\n",
      "epoch 1185, loss: 0.000167\n",
      "epoch 1186, loss: 0.000071\n",
      "epoch 1187, loss: 0.000110\n",
      "epoch 1188, loss: 0.000108\n",
      "epoch 1189, loss: 0.000085\n",
      "epoch 1190, loss: 0.000091\n",
      "epoch 1191, loss: 0.000231\n",
      "epoch 1192, loss: 0.000110\n",
      "epoch 1193, loss: 0.000172\n",
      "epoch 1194, loss: 0.000145\n",
      "epoch 1195, loss: 0.000152\n",
      "epoch 1196, loss: 0.000083\n",
      "epoch 1197, loss: 0.000041\n",
      "epoch 1198, loss: 0.000057\n",
      "epoch 1199, loss: 0.000116\n",
      "epoch 1200, loss: 0.000265\n",
      "epoch 1201, loss: 0.000065\n",
      "epoch 1202, loss: 0.000095\n",
      "epoch 1203, loss: 0.000187\n",
      "epoch 1204, loss: 0.000135\n",
      "epoch 1205, loss: 0.000088\n",
      "epoch 1206, loss: 0.000071\n",
      "epoch 1207, loss: 0.000042\n",
      "epoch 1208, loss: 0.000113\n",
      "epoch 1209, loss: 0.000050\n",
      "epoch 1210, loss: 0.000102\n",
      "epoch 1211, loss: 0.000082\n",
      "epoch 1212, loss: 0.000222\n",
      "epoch 1213, loss: 0.000105\n",
      "epoch 1214, loss: 0.000094\n",
      "epoch 1215, loss: 0.000066\n",
      "epoch 1216, loss: 0.000045\n",
      "epoch 1217, loss: 0.000119\n",
      "epoch 1218, loss: 0.000111\n",
      "epoch 1219, loss: 0.000059\n",
      "epoch 1220, loss: 0.000074\n",
      "epoch 1221, loss: 0.000206\n",
      "epoch 1222, loss: 0.000078\n",
      "epoch 1223, loss: 0.000108\n",
      "epoch 1224, loss: 0.000099\n",
      "epoch 1225, loss: 0.000110\n",
      "epoch 1226, loss: 0.000130\n",
      "epoch 1227, loss: 0.000053\n",
      "epoch 1228, loss: 0.000057\n",
      "epoch 1229, loss: 0.000159\n",
      "epoch 1230, loss: 0.000042\n",
      "epoch 1231, loss: 0.000085\n",
      "epoch 1232, loss: 0.000081\n",
      "epoch 1233, loss: 0.000094\n",
      "epoch 1234, loss: 0.000160\n",
      "epoch 1235, loss: 0.000033\n",
      "epoch 1236, loss: 0.000070\n",
      "epoch 1237, loss: 0.000120\n",
      "epoch 1238, loss: 0.000058\n",
      "epoch 1239, loss: 0.000094\n",
      "epoch 1240, loss: 0.000181\n",
      "epoch 1241, loss: 0.000100\n",
      "epoch 1242, loss: 0.000072\n",
      "epoch 1243, loss: 0.000098\n",
      "epoch 1244, loss: 0.000117\n",
      "epoch 1245, loss: 0.000108\n",
      "epoch 1246, loss: 0.000082\n",
      "epoch 1247, loss: 0.000091\n",
      "epoch 1248, loss: 0.000067\n",
      "epoch 1249, loss: 0.000060\n",
      "epoch 1250, loss: 0.000103\n",
      "epoch 1251, loss: 0.000172\n",
      "epoch 1252, loss: 0.000070\n",
      "epoch 1253, loss: 0.000083\n",
      "epoch 1254, loss: 0.000069\n",
      "epoch 1255, loss: 0.000138\n",
      "epoch 1256, loss: 0.000059\n",
      "epoch 1257, loss: 0.000093\n",
      "epoch 1258, loss: 0.000052\n",
      "epoch 1259, loss: 0.000078\n",
      "epoch 1260, loss: 0.000081\n",
      "epoch 1261, loss: 0.000073\n",
      "epoch 1262, loss: 0.000103\n",
      "epoch 1263, loss: 0.000068\n",
      "epoch 1264, loss: 0.000027\n",
      "epoch 1265, loss: 0.000065\n",
      "epoch 1266, loss: 0.000159\n",
      "epoch 1267, loss: 0.000071\n",
      "epoch 1268, loss: 0.000070\n",
      "epoch 1269, loss: 0.000112\n",
      "epoch 1270, loss: 0.000205\n",
      "epoch 1271, loss: 0.000159\n",
      "epoch 1272, loss: 0.000136\n",
      "epoch 1273, loss: 0.000039\n",
      "epoch 1274, loss: 0.000091\n",
      "epoch 1275, loss: 0.000175\n",
      "epoch 1276, loss: 0.000099\n",
      "epoch 1277, loss: 0.000153\n",
      "epoch 1278, loss: 0.000054\n",
      "epoch 1279, loss: 0.000028\n",
      "epoch 1280, loss: 0.000072\n",
      "epoch 1281, loss: 0.000165\n",
      "epoch 1282, loss: 0.000065\n",
      "epoch 1283, loss: 0.000108\n",
      "epoch 1284, loss: 0.000145\n",
      "epoch 1285, loss: 0.000120\n",
      "epoch 1286, loss: 0.000080\n",
      "epoch 1287, loss: 0.000028\n",
      "epoch 1288, loss: 0.000204\n",
      "epoch 1289, loss: 0.000086\n",
      "epoch 1290, loss: 0.000167\n",
      "epoch 1291, loss: 0.000136\n",
      "epoch 1292, loss: 0.000113\n",
      "epoch 1293, loss: 0.000096\n",
      "epoch 1294, loss: 0.000084\n",
      "epoch 1295, loss: 0.000052\n",
      "epoch 1296, loss: 0.000042\n",
      "epoch 1297, loss: 0.000144\n",
      "epoch 1298, loss: 0.000080\n",
      "epoch 1299, loss: 0.000168\n",
      "epoch 1300, loss: 0.000115\n",
      "epoch 1301, loss: 0.000135\n",
      "epoch 1302, loss: 0.000073\n",
      "epoch 1303, loss: 0.000093\n",
      "epoch 1304, loss: 0.000029\n",
      "epoch 1305, loss: 0.000126\n",
      "epoch 1306, loss: 0.000092\n",
      "epoch 1307, loss: 0.000107\n",
      "epoch 1308, loss: 0.000090\n",
      "epoch 1309, loss: 0.000046\n",
      "epoch 1310, loss: 0.000134\n",
      "epoch 1311, loss: 0.000081\n",
      "epoch 1312, loss: 0.000064\n",
      "epoch 1313, loss: 0.000115\n",
      "epoch 1314, loss: 0.000226\n",
      "epoch 1315, loss: 0.000124\n",
      "epoch 1316, loss: 0.000091\n",
      "epoch 1317, loss: 0.000067\n",
      "epoch 1318, loss: 0.000082\n",
      "epoch 1319, loss: 0.000109\n",
      "epoch 1320, loss: 0.000058\n",
      "epoch 1321, loss: 0.000098\n",
      "epoch 1322, loss: 0.000063\n",
      "epoch 1323, loss: 0.000113\n",
      "epoch 1324, loss: 0.000075\n",
      "epoch 1325, loss: 0.000089\n",
      "epoch 1326, loss: 0.000058\n",
      "epoch 1327, loss: 0.000098\n",
      "epoch 1328, loss: 0.000048\n",
      "epoch 1329, loss: 0.000131\n",
      "epoch 1330, loss: 0.000189\n",
      "epoch 1331, loss: 0.000179\n",
      "epoch 1332, loss: 0.000059\n",
      "epoch 1333, loss: 0.000087\n",
      "epoch 1334, loss: 0.000063\n",
      "epoch 1335, loss: 0.000182\n",
      "epoch 1336, loss: 0.000064\n",
      "epoch 1337, loss: 0.000162\n",
      "epoch 1338, loss: 0.000085\n",
      "epoch 1339, loss: 0.000070\n",
      "epoch 1340, loss: 0.000164\n",
      "epoch 1341, loss: 0.000097\n",
      "epoch 1342, loss: 0.000065\n",
      "epoch 1343, loss: 0.000170\n",
      "epoch 1344, loss: 0.000036\n",
      "epoch 1345, loss: 0.000075\n",
      "epoch 1346, loss: 0.000128\n",
      "epoch 1347, loss: 0.000049\n",
      "epoch 1348, loss: 0.000048\n",
      "epoch 1349, loss: 0.000083\n",
      "epoch 1350, loss: 0.000114\n",
      "epoch 1351, loss: 0.000161\n",
      "epoch 1352, loss: 0.000149\n",
      "epoch 1353, loss: 0.000083\n",
      "epoch 1354, loss: 0.000076\n",
      "epoch 1355, loss: 0.000037\n",
      "epoch 1356, loss: 0.000038\n",
      "epoch 1357, loss: 0.000074\n",
      "epoch 1358, loss: 0.000060\n",
      "epoch 1359, loss: 0.000094\n",
      "epoch 1360, loss: 0.000105\n",
      "epoch 1361, loss: 0.000108\n",
      "epoch 1362, loss: 0.000062\n",
      "epoch 1363, loss: 0.000129\n",
      "epoch 1364, loss: 0.000130\n",
      "epoch 1365, loss: 0.000135\n",
      "epoch 1366, loss: 0.000082\n",
      "epoch 1367, loss: 0.000080\n",
      "epoch 1368, loss: 0.000141\n",
      "epoch 1369, loss: 0.000052\n",
      "epoch 1370, loss: 0.000065\n",
      "epoch 1371, loss: 0.000145\n",
      "epoch 1372, loss: 0.000222\n",
      "epoch 1373, loss: 0.000127\n",
      "epoch 1374, loss: 0.000043\n",
      "epoch 1375, loss: 0.000061\n",
      "epoch 1376, loss: 0.000085\n",
      "epoch 1377, loss: 0.000086\n",
      "epoch 1378, loss: 0.000059\n",
      "epoch 1379, loss: 0.000118\n",
      "epoch 1380, loss: 0.000061\n",
      "epoch 1381, loss: 0.000171\n",
      "epoch 1382, loss: 0.000153\n",
      "epoch 1383, loss: 0.000140\n",
      "epoch 1384, loss: 0.000103\n",
      "epoch 1385, loss: 0.000171\n",
      "epoch 1386, loss: 0.000116\n",
      "epoch 1387, loss: 0.000101\n",
      "epoch 1388, loss: 0.000113\n",
      "epoch 1389, loss: 0.000137\n",
      "epoch 1390, loss: 0.000072\n",
      "epoch 1391, loss: 0.000101\n",
      "epoch 1392, loss: 0.000106\n",
      "epoch 1393, loss: 0.000104\n",
      "epoch 1394, loss: 0.000107\n",
      "epoch 1395, loss: 0.000112\n",
      "epoch 1396, loss: 0.000064\n",
      "epoch 1397, loss: 0.000113\n",
      "epoch 1398, loss: 0.000136\n",
      "epoch 1399, loss: 0.000108\n",
      "epoch 1400, loss: 0.000132\n",
      "epoch 1401, loss: 0.000133\n",
      "epoch 1402, loss: 0.000068\n",
      "epoch 1403, loss: 0.000073\n",
      "epoch 1404, loss: 0.000092\n",
      "epoch 1405, loss: 0.000133\n",
      "epoch 1406, loss: 0.000042\n",
      "epoch 1407, loss: 0.000093\n",
      "epoch 1408, loss: 0.000113\n",
      "epoch 1409, loss: 0.000077\n",
      "epoch 1410, loss: 0.000094\n",
      "epoch 1411, loss: 0.000095\n",
      "epoch 1412, loss: 0.000054\n",
      "epoch 1413, loss: 0.000145\n",
      "epoch 1414, loss: 0.000071\n",
      "epoch 1415, loss: 0.000148\n",
      "epoch 1416, loss: 0.000053\n",
      "epoch 1417, loss: 0.000113\n",
      "epoch 1418, loss: 0.000075\n",
      "epoch 1419, loss: 0.000073\n",
      "epoch 1420, loss: 0.000101\n",
      "epoch 1421, loss: 0.000095\n",
      "epoch 1422, loss: 0.000109\n",
      "epoch 1423, loss: 0.000107\n",
      "epoch 1424, loss: 0.000190\n",
      "epoch 1425, loss: 0.000102\n",
      "epoch 1426, loss: 0.000112\n",
      "epoch 1427, loss: 0.000089\n",
      "epoch 1428, loss: 0.000056\n",
      "epoch 1429, loss: 0.000130\n",
      "epoch 1430, loss: 0.000079\n",
      "epoch 1431, loss: 0.000082\n",
      "epoch 1432, loss: 0.000105\n",
      "epoch 1433, loss: 0.000095\n",
      "epoch 1434, loss: 0.000099\n",
      "epoch 1435, loss: 0.000119\n",
      "epoch 1436, loss: 0.000151\n",
      "epoch 1437, loss: 0.000052\n",
      "epoch 1438, loss: 0.000074\n",
      "epoch 1439, loss: 0.000086\n",
      "epoch 1440, loss: 0.000121\n",
      "epoch 1441, loss: 0.000073\n",
      "epoch 1442, loss: 0.000185\n",
      "epoch 1443, loss: 0.000048\n",
      "epoch 1444, loss: 0.000048\n",
      "epoch 1445, loss: 0.000124\n",
      "epoch 1446, loss: 0.000183\n",
      "epoch 1447, loss: 0.000078\n",
      "epoch 1448, loss: 0.000050\n",
      "epoch 1449, loss: 0.000191\n",
      "epoch 1450, loss: 0.000122\n",
      "epoch 1451, loss: 0.000131\n",
      "epoch 1452, loss: 0.000096\n",
      "epoch 1453, loss: 0.000098\n",
      "epoch 1454, loss: 0.000081\n",
      "epoch 1455, loss: 0.000097\n",
      "epoch 1456, loss: 0.000034\n",
      "epoch 1457, loss: 0.000069\n",
      "epoch 1458, loss: 0.000072\n",
      "epoch 1459, loss: 0.000143\n",
      "epoch 1460, loss: 0.000053\n",
      "epoch 1461, loss: 0.000073\n",
      "epoch 1462, loss: 0.000105\n",
      "epoch 1463, loss: 0.000147\n",
      "epoch 1464, loss: 0.000073\n",
      "epoch 1465, loss: 0.000040\n",
      "epoch 1466, loss: 0.000108\n",
      "epoch 1467, loss: 0.000068\n",
      "epoch 1468, loss: 0.000157\n",
      "epoch 1469, loss: 0.000146\n",
      "epoch 1470, loss: 0.000089\n",
      "epoch 1471, loss: 0.000067\n",
      "epoch 1472, loss: 0.000069\n",
      "epoch 1473, loss: 0.000083\n",
      "epoch 1474, loss: 0.000219\n",
      "epoch 1475, loss: 0.000113\n",
      "epoch 1476, loss: 0.000099\n",
      "epoch 1477, loss: 0.000164\n",
      "epoch 1478, loss: 0.000170\n",
      "epoch 1479, loss: 0.000081\n",
      "epoch 1480, loss: 0.000105\n",
      "epoch 1481, loss: 0.000095\n",
      "epoch 1482, loss: 0.000073\n",
      "epoch 1483, loss: 0.000139\n",
      "epoch 1484, loss: 0.000069\n",
      "epoch 1485, loss: 0.000138\n",
      "epoch 1486, loss: 0.000114\n",
      "epoch 1487, loss: 0.000085\n",
      "epoch 1488, loss: 0.000070\n",
      "epoch 1489, loss: 0.000066\n",
      "epoch 1490, loss: 0.000212\n",
      "epoch 1491, loss: 0.000073\n",
      "epoch 1492, loss: 0.000081\n",
      "epoch 1493, loss: 0.000094\n",
      "epoch 1494, loss: 0.000090\n",
      "epoch 1495, loss: 0.000081\n",
      "epoch 1496, loss: 0.000036\n",
      "epoch 1497, loss: 0.000156\n",
      "epoch 1498, loss: 0.000069\n",
      "epoch 1499, loss: 0.000205\n",
      "epoch 1500, loss: 0.000105\n",
      "epoch 1501, loss: 0.000052\n",
      "epoch 1502, loss: 0.000130\n",
      "epoch 1503, loss: 0.000142\n",
      "epoch 1504, loss: 0.000066\n",
      "epoch 1505, loss: 0.000076\n",
      "epoch 1506, loss: 0.000073\n",
      "epoch 1507, loss: 0.000101\n",
      "epoch 1508, loss: 0.000103\n",
      "epoch 1509, loss: 0.000108\n",
      "epoch 1510, loss: 0.000050\n",
      "epoch 1511, loss: 0.000084\n",
      "epoch 1512, loss: 0.000109\n",
      "epoch 1513, loss: 0.000098\n",
      "epoch 1514, loss: 0.000192\n",
      "epoch 1515, loss: 0.000053\n",
      "epoch 1516, loss: 0.000090\n",
      "epoch 1517, loss: 0.000058\n",
      "epoch 1518, loss: 0.000133\n",
      "epoch 1519, loss: 0.000195\n",
      "epoch 1520, loss: 0.000129\n",
      "epoch 1521, loss: 0.000059\n",
      "epoch 1522, loss: 0.000128\n",
      "epoch 1523, loss: 0.000065\n",
      "epoch 1524, loss: 0.000149\n",
      "epoch 1525, loss: 0.000020\n",
      "epoch 1526, loss: 0.000031\n",
      "epoch 1527, loss: 0.000195\n",
      "epoch 1528, loss: 0.000113\n",
      "epoch 1529, loss: 0.000087\n",
      "epoch 1530, loss: 0.000130\n",
      "epoch 1531, loss: 0.000082\n",
      "epoch 1532, loss: 0.000114\n",
      "epoch 1533, loss: 0.000083\n",
      "epoch 1534, loss: 0.000066\n",
      "epoch 1535, loss: 0.000118\n",
      "epoch 1536, loss: 0.000093\n",
      "epoch 1537, loss: 0.000125\n",
      "epoch 1538, loss: 0.000188\n",
      "epoch 1539, loss: 0.000056\n",
      "epoch 1540, loss: 0.000047\n",
      "epoch 1541, loss: 0.000102\n",
      "epoch 1542, loss: 0.000051\n",
      "epoch 1543, loss: 0.000111\n",
      "epoch 1544, loss: 0.000137\n",
      "epoch 1545, loss: 0.000053\n",
      "epoch 1546, loss: 0.000112\n",
      "epoch 1547, loss: 0.000202\n",
      "epoch 1548, loss: 0.000127\n",
      "epoch 1549, loss: 0.000096\n",
      "epoch 1550, loss: 0.000062\n",
      "epoch 1551, loss: 0.000107\n",
      "epoch 1552, loss: 0.000144\n",
      "epoch 1553, loss: 0.000107\n",
      "epoch 1554, loss: 0.000133\n",
      "epoch 1555, loss: 0.000126\n",
      "epoch 1556, loss: 0.000073\n",
      "epoch 1557, loss: 0.000065\n",
      "epoch 1558, loss: 0.000115\n",
      "epoch 1559, loss: 0.000095\n",
      "epoch 1560, loss: 0.000031\n",
      "epoch 1561, loss: 0.000141\n",
      "epoch 1562, loss: 0.000140\n",
      "epoch 1563, loss: 0.000095\n",
      "epoch 1564, loss: 0.000069\n",
      "epoch 1565, loss: 0.000115\n",
      "epoch 1566, loss: 0.000106\n",
      "epoch 1567, loss: 0.000055\n",
      "epoch 1568, loss: 0.000041\n",
      "epoch 1569, loss: 0.000039\n",
      "epoch 1570, loss: 0.000061\n",
      "epoch 1571, loss: 0.000077\n",
      "epoch 1572, loss: 0.000050\n",
      "epoch 1573, loss: 0.000069\n",
      "epoch 1574, loss: 0.000165\n",
      "epoch 1575, loss: 0.000093\n",
      "epoch 1576, loss: 0.000032\n",
      "epoch 1577, loss: 0.000060\n",
      "epoch 1578, loss: 0.000029\n",
      "epoch 1579, loss: 0.000209\n",
      "epoch 1580, loss: 0.000111\n",
      "epoch 1581, loss: 0.000082\n",
      "epoch 1582, loss: 0.000034\n",
      "epoch 1583, loss: 0.000111\n",
      "epoch 1584, loss: 0.000162\n",
      "epoch 1585, loss: 0.000069\n",
      "epoch 1586, loss: 0.000121\n",
      "epoch 1587, loss: 0.000179\n",
      "epoch 1588, loss: 0.000027\n",
      "epoch 1589, loss: 0.000099\n",
      "epoch 1590, loss: 0.000066\n",
      "epoch 1591, loss: 0.000103\n",
      "epoch 1592, loss: 0.000159\n",
      "epoch 1593, loss: 0.000111\n",
      "epoch 1594, loss: 0.000141\n",
      "epoch 1595, loss: 0.000120\n",
      "epoch 1596, loss: 0.000068\n",
      "epoch 1597, loss: 0.000069\n",
      "epoch 1598, loss: 0.000121\n",
      "epoch 1599, loss: 0.000076\n",
      "epoch 1600, loss: 0.000053\n",
      "epoch 1601, loss: 0.000177\n",
      "epoch 1602, loss: 0.000046\n",
      "epoch 1603, loss: 0.000110\n",
      "epoch 1604, loss: 0.000050\n",
      "epoch 1605, loss: 0.000134\n",
      "epoch 1606, loss: 0.000022\n",
      "epoch 1607, loss: 0.000163\n",
      "epoch 1608, loss: 0.000071\n",
      "epoch 1609, loss: 0.000198\n",
      "epoch 1610, loss: 0.000087\n",
      "epoch 1611, loss: 0.000090\n",
      "epoch 1612, loss: 0.000067\n",
      "epoch 1613, loss: 0.000081\n",
      "epoch 1614, loss: 0.000096\n",
      "epoch 1615, loss: 0.000084\n",
      "epoch 1616, loss: 0.000185\n",
      "epoch 1617, loss: 0.000070\n",
      "epoch 1618, loss: 0.000106\n",
      "epoch 1619, loss: 0.000063\n",
      "epoch 1620, loss: 0.000074\n",
      "epoch 1621, loss: 0.000069\n",
      "epoch 1622, loss: 0.000148\n",
      "epoch 1623, loss: 0.000168\n",
      "epoch 1624, loss: 0.000085\n",
      "epoch 1625, loss: 0.000145\n",
      "epoch 1626, loss: 0.000123\n",
      "epoch 1627, loss: 0.000073\n",
      "epoch 1628, loss: 0.000123\n",
      "epoch 1629, loss: 0.000117\n",
      "epoch 1630, loss: 0.000063\n",
      "epoch 1631, loss: 0.000080\n",
      "epoch 1632, loss: 0.000149\n",
      "epoch 1633, loss: 0.000075\n",
      "epoch 1634, loss: 0.000045\n",
      "epoch 1635, loss: 0.000096\n",
      "epoch 1636, loss: 0.000045\n",
      "epoch 1637, loss: 0.000122\n",
      "epoch 1638, loss: 0.000079\n",
      "epoch 1639, loss: 0.000107\n",
      "epoch 1640, loss: 0.000053\n",
      "epoch 1641, loss: 0.000110\n",
      "epoch 1642, loss: 0.000071\n",
      "epoch 1643, loss: 0.000127\n",
      "epoch 1644, loss: 0.000061\n",
      "epoch 1645, loss: 0.000064\n",
      "epoch 1646, loss: 0.000066\n",
      "epoch 1647, loss: 0.000104\n",
      "epoch 1648, loss: 0.000094\n",
      "epoch 1649, loss: 0.000226\n",
      "epoch 1650, loss: 0.000035\n",
      "epoch 1651, loss: 0.000093\n",
      "epoch 1652, loss: 0.000088\n",
      "epoch 1653, loss: 0.000031\n",
      "epoch 1654, loss: 0.000115\n",
      "epoch 1655, loss: 0.000047\n",
      "epoch 1656, loss: 0.000236\n",
      "epoch 1657, loss: 0.000127\n",
      "epoch 1658, loss: 0.000092\n",
      "epoch 1659, loss: 0.000123\n",
      "epoch 1660, loss: 0.000293\n",
      "epoch 1661, loss: 0.000125\n",
      "epoch 1662, loss: 0.000144\n",
      "epoch 1663, loss: 0.000215\n",
      "epoch 1664, loss: 0.000145\n",
      "epoch 1665, loss: 0.000117\n",
      "epoch 1666, loss: 0.000076\n",
      "epoch 1667, loss: 0.000097\n",
      "epoch 1668, loss: 0.000110\n",
      "epoch 1669, loss: 0.000177\n",
      "epoch 1670, loss: 0.000135\n",
      "epoch 1671, loss: 0.000129\n",
      "epoch 1672, loss: 0.000197\n",
      "epoch 1673, loss: 0.000077\n",
      "epoch 1674, loss: 0.000115\n",
      "epoch 1675, loss: 0.000065\n",
      "epoch 1676, loss: 0.000068\n",
      "epoch 1677, loss: 0.000161\n",
      "epoch 1678, loss: 0.000087\n",
      "epoch 1679, loss: 0.000025\n",
      "epoch 1680, loss: 0.000152\n",
      "epoch 1681, loss: 0.000133\n",
      "epoch 1682, loss: 0.000057\n",
      "epoch 1683, loss: 0.000114\n",
      "epoch 1684, loss: 0.000245\n",
      "epoch 1685, loss: 0.000049\n",
      "epoch 1686, loss: 0.000078\n",
      "epoch 1687, loss: 0.000091\n",
      "epoch 1688, loss: 0.000042\n",
      "epoch 1689, loss: 0.000126\n",
      "epoch 1690, loss: 0.000149\n",
      "epoch 1691, loss: 0.000223\n",
      "epoch 1692, loss: 0.000053\n",
      "epoch 1693, loss: 0.000187\n",
      "epoch 1694, loss: 0.000048\n",
      "epoch 1695, loss: 0.000064\n",
      "epoch 1696, loss: 0.000085\n",
      "epoch 1697, loss: 0.000120\n",
      "epoch 1698, loss: 0.000171\n",
      "epoch 1699, loss: 0.000205\n",
      "epoch 1700, loss: 0.000111\n",
      "epoch 1701, loss: 0.000033\n",
      "epoch 1702, loss: 0.000117\n",
      "epoch 1703, loss: 0.000064\n",
      "epoch 1704, loss: 0.000077\n",
      "epoch 1705, loss: 0.000083\n",
      "epoch 1706, loss: 0.000106\n",
      "epoch 1707, loss: 0.000054\n",
      "epoch 1708, loss: 0.000126\n",
      "epoch 1709, loss: 0.000070\n",
      "epoch 1710, loss: 0.000131\n",
      "epoch 1711, loss: 0.000061\n",
      "epoch 1712, loss: 0.000081\n",
      "epoch 1713, loss: 0.000143\n",
      "epoch 1714, loss: 0.000151\n",
      "epoch 1715, loss: 0.000078\n",
      "epoch 1716, loss: 0.000035\n",
      "epoch 1717, loss: 0.000023\n",
      "epoch 1718, loss: 0.000058\n",
      "epoch 1719, loss: 0.000094\n",
      "epoch 1720, loss: 0.000112\n",
      "epoch 1721, loss: 0.000044\n",
      "epoch 1722, loss: 0.000062\n",
      "epoch 1723, loss: 0.000355\n",
      "epoch 1724, loss: 0.000124\n",
      "epoch 1725, loss: 0.000074\n",
      "epoch 1726, loss: 0.000074\n",
      "epoch 1727, loss: 0.000168\n",
      "epoch 1728, loss: 0.000145\n",
      "epoch 1729, loss: 0.000061\n",
      "epoch 1730, loss: 0.000059\n",
      "epoch 1731, loss: 0.000144\n",
      "epoch 1732, loss: 0.000188\n",
      "epoch 1733, loss: 0.000100\n",
      "epoch 1734, loss: 0.000077\n",
      "epoch 1735, loss: 0.000099\n",
      "epoch 1736, loss: 0.000076\n",
      "epoch 1737, loss: 0.000043\n",
      "epoch 1738, loss: 0.000122\n",
      "epoch 1739, loss: 0.000144\n",
      "epoch 1740, loss: 0.000071\n",
      "epoch 1741, loss: 0.000120\n",
      "epoch 1742, loss: 0.000077\n",
      "epoch 1743, loss: 0.000064\n",
      "epoch 1744, loss: 0.000095\n",
      "epoch 1745, loss: 0.000084\n",
      "epoch 1746, loss: 0.000067\n",
      "epoch 1747, loss: 0.000150\n",
      "epoch 1748, loss: 0.000129\n",
      "epoch 1749, loss: 0.000150\n",
      "epoch 1750, loss: 0.000054\n",
      "epoch 1751, loss: 0.000042\n",
      "epoch 1752, loss: 0.000140\n",
      "epoch 1753, loss: 0.000111\n",
      "epoch 1754, loss: 0.000085\n",
      "epoch 1755, loss: 0.000102\n",
      "epoch 1756, loss: 0.000069\n",
      "epoch 1757, loss: 0.000054\n",
      "epoch 1758, loss: 0.000178\n",
      "epoch 1759, loss: 0.000124\n",
      "epoch 1760, loss: 0.000064\n",
      "epoch 1761, loss: 0.000075\n",
      "epoch 1762, loss: 0.000135\n",
      "epoch 1763, loss: 0.000050\n",
      "epoch 1764, loss: 0.000083\n",
      "epoch 1765, loss: 0.000062\n",
      "epoch 1766, loss: 0.000102\n",
      "epoch 1767, loss: 0.000189\n",
      "epoch 1768, loss: 0.000163\n",
      "epoch 1769, loss: 0.000115\n",
      "epoch 1770, loss: 0.000150\n",
      "epoch 1771, loss: 0.000110\n",
      "epoch 1772, loss: 0.000085\n",
      "epoch 1773, loss: 0.000125\n",
      "epoch 1774, loss: 0.000087\n",
      "epoch 1775, loss: 0.000080\n",
      "epoch 1776, loss: 0.000086\n",
      "epoch 1777, loss: 0.000118\n",
      "epoch 1778, loss: 0.000046\n",
      "epoch 1779, loss: 0.000090\n",
      "epoch 1780, loss: 0.000104\n",
      "epoch 1781, loss: 0.000056\n",
      "epoch 1782, loss: 0.000121\n",
      "epoch 1783, loss: 0.000286\n",
      "epoch 1784, loss: 0.000115\n",
      "epoch 1785, loss: 0.000085\n",
      "epoch 1786, loss: 0.000054\n",
      "epoch 1787, loss: 0.000069\n",
      "epoch 1788, loss: 0.000061\n",
      "epoch 1789, loss: 0.000106\n",
      "epoch 1790, loss: 0.000189\n",
      "epoch 1791, loss: 0.000054\n",
      "epoch 1792, loss: 0.000072\n",
      "epoch 1793, loss: 0.000110\n",
      "epoch 1794, loss: 0.000045\n",
      "epoch 1795, loss: 0.000056\n",
      "epoch 1796, loss: 0.000120\n",
      "epoch 1797, loss: 0.000138\n",
      "epoch 1798, loss: 0.000062\n",
      "epoch 1799, loss: 0.000153\n",
      "epoch 1800, loss: 0.000065\n",
      "epoch 1801, loss: 0.000126\n",
      "epoch 1802, loss: 0.000074\n",
      "epoch 1803, loss: 0.000140\n",
      "epoch 1804, loss: 0.000067\n",
      "epoch 1805, loss: 0.000161\n",
      "epoch 1806, loss: 0.000083\n",
      "epoch 1807, loss: 0.000044\n",
      "epoch 1808, loss: 0.000101\n",
      "epoch 1809, loss: 0.000104\n",
      "epoch 1810, loss: 0.000092\n",
      "epoch 1811, loss: 0.000141\n",
      "epoch 1812, loss: 0.000108\n",
      "epoch 1813, loss: 0.000077\n",
      "epoch 1814, loss: 0.000093\n",
      "epoch 1815, loss: 0.000168\n",
      "epoch 1816, loss: 0.000165\n",
      "epoch 1817, loss: 0.000104\n",
      "epoch 1818, loss: 0.000065\n",
      "epoch 1819, loss: 0.000140\n",
      "epoch 1820, loss: 0.000074\n",
      "epoch 1821, loss: 0.000047\n",
      "epoch 1822, loss: 0.000196\n",
      "epoch 1823, loss: 0.000096\n",
      "epoch 1824, loss: 0.000100\n",
      "epoch 1825, loss: 0.000095\n",
      "epoch 1826, loss: 0.000067\n",
      "epoch 1827, loss: 0.000077\n",
      "epoch 1828, loss: 0.000053\n",
      "epoch 1829, loss: 0.000089\n",
      "epoch 1830, loss: 0.000095\n",
      "epoch 1831, loss: 0.000111\n",
      "epoch 1832, loss: 0.000139\n",
      "epoch 1833, loss: 0.000146\n",
      "epoch 1834, loss: 0.000078\n",
      "epoch 1835, loss: 0.000085\n",
      "epoch 1836, loss: 0.000089\n",
      "epoch 1837, loss: 0.000176\n",
      "epoch 1838, loss: 0.000115\n",
      "epoch 1839, loss: 0.000072\n",
      "epoch 1840, loss: 0.000111\n",
      "epoch 1841, loss: 0.000061\n",
      "epoch 1842, loss: 0.000128\n",
      "epoch 1843, loss: 0.000056\n",
      "epoch 1844, loss: 0.000078\n",
      "epoch 1845, loss: 0.000067\n",
      "epoch 1846, loss: 0.000059\n",
      "epoch 1847, loss: 0.000082\n",
      "epoch 1848, loss: 0.000118\n",
      "epoch 1849, loss: 0.000098\n",
      "epoch 1850, loss: 0.000061\n",
      "epoch 1851, loss: 0.000133\n",
      "epoch 1852, loss: 0.000074\n",
      "epoch 1853, loss: 0.000187\n",
      "epoch 1854, loss: 0.000176\n",
      "epoch 1855, loss: 0.000045\n",
      "epoch 1856, loss: 0.000046\n",
      "epoch 1857, loss: 0.000141\n",
      "epoch 1858, loss: 0.000066\n",
      "epoch 1859, loss: 0.000071\n",
      "epoch 1860, loss: 0.000162\n",
      "epoch 1861, loss: 0.000142\n",
      "epoch 1862, loss: 0.000072\n",
      "epoch 1863, loss: 0.000103\n",
      "epoch 1864, loss: 0.000081\n",
      "epoch 1865, loss: 0.000164\n",
      "epoch 1866, loss: 0.000026\n",
      "epoch 1867, loss: 0.000107\n",
      "epoch 1868, loss: 0.000041\n",
      "epoch 1869, loss: 0.000054\n",
      "epoch 1870, loss: 0.000048\n",
      "epoch 1871, loss: 0.000188\n",
      "epoch 1872, loss: 0.000165\n",
      "epoch 1873, loss: 0.000234\n",
      "epoch 1874, loss: 0.000066\n",
      "epoch 1875, loss: 0.000052\n",
      "epoch 1876, loss: 0.000079\n",
      "epoch 1877, loss: 0.000065\n",
      "epoch 1878, loss: 0.000101\n",
      "epoch 1879, loss: 0.000208\n",
      "epoch 1880, loss: 0.000066\n",
      "epoch 1881, loss: 0.000051\n",
      "epoch 1882, loss: 0.000117\n",
      "epoch 1883, loss: 0.000050\n",
      "epoch 1884, loss: 0.000049\n",
      "epoch 1885, loss: 0.000126\n",
      "epoch 1886, loss: 0.000111\n",
      "epoch 1887, loss: 0.000246\n",
      "epoch 1888, loss: 0.000171\n",
      "epoch 1889, loss: 0.000050\n",
      "epoch 1890, loss: 0.000158\n",
      "epoch 1891, loss: 0.000074\n",
      "epoch 1892, loss: 0.000083\n",
      "epoch 1893, loss: 0.000224\n",
      "epoch 1894, loss: 0.000081\n",
      "epoch 1895, loss: 0.000075\n",
      "epoch 1896, loss: 0.000103\n",
      "epoch 1897, loss: 0.000216\n",
      "epoch 1898, loss: 0.000107\n",
      "epoch 1899, loss: 0.000200\n",
      "epoch 1900, loss: 0.000117\n",
      "epoch 1901, loss: 0.000071\n",
      "epoch 1902, loss: 0.000111\n",
      "epoch 1903, loss: 0.000080\n",
      "epoch 1904, loss: 0.000090\n",
      "epoch 1905, loss: 0.000037\n",
      "epoch 1906, loss: 0.000082\n",
      "epoch 1907, loss: 0.000109\n",
      "epoch 1908, loss: 0.000111\n",
      "epoch 1909, loss: 0.000123\n",
      "epoch 1910, loss: 0.000126\n",
      "epoch 1911, loss: 0.000061\n",
      "epoch 1912, loss: 0.000107\n",
      "epoch 1913, loss: 0.000164\n",
      "epoch 1914, loss: 0.000165\n",
      "epoch 1915, loss: 0.000101\n",
      "epoch 1916, loss: 0.000116\n",
      "epoch 1917, loss: 0.000227\n",
      "epoch 1918, loss: 0.000164\n",
      "epoch 1919, loss: 0.000077\n",
      "epoch 1920, loss: 0.000084\n",
      "epoch 1921, loss: 0.000055\n",
      "epoch 1922, loss: 0.000091\n",
      "epoch 1923, loss: 0.000054\n",
      "epoch 1924, loss: 0.000110\n",
      "epoch 1925, loss: 0.000162\n",
      "epoch 1926, loss: 0.000159\n",
      "epoch 1927, loss: 0.000222\n",
      "epoch 1928, loss: 0.000109\n",
      "epoch 1929, loss: 0.000173\n",
      "epoch 1930, loss: 0.000139\n",
      "epoch 1931, loss: 0.000083\n",
      "epoch 1932, loss: 0.000091\n",
      "epoch 1933, loss: 0.000095\n",
      "epoch 1934, loss: 0.000066\n",
      "epoch 1935, loss: 0.000134\n",
      "epoch 1936, loss: 0.000068\n",
      "epoch 1937, loss: 0.000135\n",
      "epoch 1938, loss: 0.000169\n",
      "epoch 1939, loss: 0.000162\n",
      "epoch 1940, loss: 0.000105\n",
      "epoch 1941, loss: 0.000065\n",
      "epoch 1942, loss: 0.000142\n",
      "epoch 1943, loss: 0.000075\n",
      "epoch 1944, loss: 0.000226\n",
      "epoch 1945, loss: 0.000114\n",
      "epoch 1946, loss: 0.000160\n",
      "epoch 1947, loss: 0.000050\n",
      "epoch 1948, loss: 0.000247\n",
      "epoch 1949, loss: 0.000136\n",
      "epoch 1950, loss: 0.000090\n",
      "epoch 1951, loss: 0.000065\n",
      "epoch 1952, loss: 0.000032\n",
      "epoch 1953, loss: 0.000101\n",
      "epoch 1954, loss: 0.000152\n",
      "epoch 1955, loss: 0.000151\n",
      "epoch 1956, loss: 0.000161\n",
      "epoch 1957, loss: 0.000041\n",
      "epoch 1958, loss: 0.000063\n",
      "epoch 1959, loss: 0.000109\n",
      "epoch 1960, loss: 0.000127\n",
      "epoch 1961, loss: 0.000135\n",
      "epoch 1962, loss: 0.000071\n",
      "epoch 1963, loss: 0.000090\n",
      "epoch 1964, loss: 0.000089\n",
      "epoch 1965, loss: 0.000127\n",
      "epoch 1966, loss: 0.000048\n",
      "epoch 1967, loss: 0.000079\n",
      "epoch 1968, loss: 0.000048\n",
      "epoch 1969, loss: 0.000078\n",
      "epoch 1970, loss: 0.000081\n",
      "epoch 1971, loss: 0.000058\n",
      "epoch 1972, loss: 0.000107\n",
      "epoch 1973, loss: 0.000106\n",
      "epoch 1974, loss: 0.000105\n",
      "epoch 1975, loss: 0.000138\n",
      "epoch 1976, loss: 0.000060\n",
      "epoch 1977, loss: 0.000070\n",
      "epoch 1978, loss: 0.000047\n",
      "epoch 1979, loss: 0.000134\n",
      "epoch 1980, loss: 0.000040\n",
      "epoch 1981, loss: 0.000107\n",
      "epoch 1982, loss: 0.000083\n",
      "epoch 1983, loss: 0.000115\n",
      "epoch 1984, loss: 0.000055\n",
      "epoch 1985, loss: 0.000043\n",
      "epoch 1986, loss: 0.000046\n",
      "epoch 1987, loss: 0.000227\n",
      "epoch 1988, loss: 0.000107\n",
      "epoch 1989, loss: 0.000174\n",
      "epoch 1990, loss: 0.000080\n",
      "epoch 1991, loss: 0.000112\n",
      "epoch 1992, loss: 0.000097\n",
      "epoch 1993, loss: 0.000073\n",
      "epoch 1994, loss: 0.000162\n",
      "epoch 1995, loss: 0.000146\n",
      "epoch 1996, loss: 0.000041\n",
      "epoch 1997, loss: 0.000119\n",
      "epoch 1998, loss: 0.000068\n",
      "epoch 1999, loss: 0.000086\n",
      "epoch 2000, loss: 0.000045\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 2000\n",
    "for epoch in range(1, num_epoch + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1,1))\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] Parameter containing:\n",
      "tensor([[ 2.0000, -3.4000]], requires_grad=True)\n",
      "4.2 Parameter containing:\n",
      "tensor([4.1999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dense = net[0]\n",
    "print(true_w, dense.weight)\n",
    "print(true_b, dense.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9bb90c8c31d02c70dc2c25559c9d82f9e11e69c39856b10e40706be7daafd48"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
